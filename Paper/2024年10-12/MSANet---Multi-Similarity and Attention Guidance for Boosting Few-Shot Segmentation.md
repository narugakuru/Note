---
title: MSANet---Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation
tags:
  - Survey
  - FSS
  - Attention
date created: 2024-10-15
date modified: 2024-10-25
NotionID-Paper: 129e05f0-ba91-8132-a0cd-c8350e48ecc8
link-Paper: https://hikari-note.notion.site/MSANet-Multi-Similarity-and-Attention-Guidance-for-Boosting-Few-Shot-Segmentation-129e05f0ba918132a0cdc8350e48ecc8
---

# 综述

- 我们提出一个多层相似度模块，以在支持图像与查询图像之间建立信息丰富的视觉对应关系。
- 我们提出了一种简单但有效的注意力模块，利用支持图像及其对应掩码来更好地理解与类别相关的信息。
- MSANet 超越了现有的 FSS 网络，在 PASCAL5i [45] 和 COCO-20 i [39] FSS 基准测试的 1-shot 和 5-shot 设置下展示了最先进的（SOTA）结果。

# FSS框架现状

FSS 最普遍的方法是基于度量的原型学习[51]。参考图 1 的上半部分，通过掩码平均池化（MAP）[67]生成单个或多个类代表性原型向量。特征处理网络利用这些类代表性原型向量在查询图像中分割目标对象。许多研究者尝试通过采用不同机制从原型向量中获取更多指导，例如 PANet[55]、PFENet[51]、SG-One Net[67]、CANet[65]、ASGNet[22]。然而，此类原型网络由于掩码平均池化操作可能会丢失图像的详细空间信息。在此背景下，我们提出了一种多相似性与注意力网络（MSANet），包含两个引导模块。参考图 1 的下半部分，该网络包括一个多层相似性模块和一个注意力模块。期望这两个模块能支持原型学习范式，并引导 MSANet 实现精细分割。

少样本学习：为解决这些问题，引入了少样本学习（FSL），旨在仅通过少量标注样本理解未见类别。FSL 方法可进一步细分为三个分支：（i）基于优化的方法[42, 11, 19]，（ii）基于增强的方法[6, 7]，以及（iii）基于度量的方法[48, 23, 50]。基于优化的方法提出梯度更新策略以克服数据偏差并提升模型泛化能力。基于增强的方法通过生成合成训练图像来应对数据匮乏问题。我们的工作与基于度量的方法密切相关，这些方法旨在学习一个通用度量函数，用于计算查询图像与支持图像之间的距离。基于度量的方法取得了显著进展。其中，匹配网络[53]利用一种称为“情节”的特殊小批量来匹配训练和测试环境。关系网络[50]将查询和支持图像转换为 1x1 向量，然后基于余弦相似度（CS）进行分类。此外，原型网络[48]直接利用特征表示（即...通过全局平均池化操作计算得到的原型，被提出。

少样本分割：Shaban 等人[45]提出了 OSLSM，这是 FSS 的开创性工作之一，用于生成查询图像分割的分类器权重。第一个分支以支持图像为输入并生成参数向量，第二个分支则利用这些参数以及查询图像生成分割掩码作为输出。随后，引入了原型学习范式[48]，以更好地从支持图像和查询图像中提取信息。SG-One[67]引入了掩码平均池化操作来计算类代表性原型向量，从而生成空间相似度图。CANet[65]提出了两个密集比较网络，并配备了一个迭代优化模块。PFENet[51]在没有可训练参数的情况下计算高层特征的 CS，以创建先验掩码，并引入了一个特征增强模块。ASGNet[22]则采用了一种超像素引导的聚类方法，从支持图像中提取多个原型，并使用分配策略重构支持特征图，而非原型扩展。然而，大多数原型学习方法可能导致空间结构损失。 为了充分利用前景对象的特征，在利用类别代表性原型向量方面仍有改进空间。另一方面，寻找视觉对应关系并处理相关张量在 FSS 中显示出显著成果[37, 38, 36]。HSNet[36]经过训练，能够压缩密集特征相关张量并通过高维卷积将其转换为分割掩码。然而，高维卷积（4D 卷积）具有较高的空间和时间复杂度。为提取轻量级 CNN 特征，DENet[30]引入了一个引导注意力模块，借鉴传统注意力机制来估计新型分类器的权重。Tao Hu 等人[17]提出了一种基于注意力的多上下文引导网络，该网络融合了从小到大的尺度上下文信息，以全局引导查询分支。BAM[20]则为 FSS 引入了一种新方法，它利用了在基础类别上训练的监督模型的额外模块。该监督模型从查询图像中预测基础类别，并帮助元学习者抑制错误预测。 受视觉对应关系和注意力机制最新进展的启发，我们在原型网络的背景下提出了一种多层相似性模块和一种轻量级注意力模块，以将 FSS 网络推向新的高度。

现有网络与 MSANet 之间的元学习者比较。主要区别在于前者仅使用类别代表性原型向量，而 MSANet 则包含用于视觉对应的多相似性模块和用于目标类别聚焦的注意力模块。其余网络结构与 BAM[20]的架构相同。

# 架构

![](Paper/2024年10-12/attachments/Pasted%20image%2020241016143241.png)

![](Paper/2024年10-12/attachments/Pasted%20image%2020241016143222.png)
![](Paper/2024年10-12/attachments/Pasted%20image%2020241016145653.png)

## 架构解析

模型架构：图 2 展示了 MSANet 的架构。首先，从预训练的主干网络中提取查询图像和支持图像的特征。
利用从块 2 和块 3 提取的支持特征及其对应掩码，找到一个类代表性的原型向量 Vs 。这些特征及其掩码被输入注意力模块，以寻找注意力特征图。
注意力模块首先对支持特征进行掩码处理，然后使用简单的卷积网络生成前景聚焦的注意力特征图。从块 4 生成的查询特征和支持特征用于生成先验掩码 Mp​r ，遵循[51]的方法。同时，从块 2、3 和 4 提取的查询图像和支持图像的所有特征被用于通过多相似性模块生成视觉对应关系。在该模块中，计算了多层查询特征与支持特征之间的 CS 距离，并对特征应用了简单的 1 × 1 C​o​n​v 操作。该模块的详细信息在第 4.1 节中提及。
生成的视觉对应关系、注意力图、先验掩码和原型向量连同查询特征被输入到特征增强 ASPP 模块中。为聚焦于特征的近似信息，采用了 ASPP 模块的扩张版本。从 ASPP 模块获取丰富特征后，使用一个简单的卷积块进行特征处理。由 3 个 × 3 C​o​n​v 和 1 个 × 1 C​o​n​v 组成的分类器头用于生成二元元预测掩码。卷积块和分类器头的结构如图 3 所示。最后，通过集成模块，元学习器的输出与基础学习器 1 结合进行精炼。

```
拼接前：

query_backbone_layers[2]：(bs, 512, h/8, w/8)
query_backbone_layers[3]：(bs, 1024, h/16, w/16)

query_feat：查询图像的特征图。
concat_feat：支持图像的特征图，通过 supp_pro 扩展到与 query_feat 相同的尺寸。
hyper_final：超特征图，通过 hyper_final 模块处理得到。
corr_query_mask：查询图像与支持图像之间的相关性特征图。
gam：通过 Attention 模块处理得到的特征图。

query_feat：(bs, 256, h/16, w/16)
concat_feat：(bs, 256, h/16, w/16)
hyper_final：(bs, 64, h/16, w/16)
corr_query_mask：(bs, 1, h/16, w/16)
gam：(bs, 256, h/16, w/16)

query_feat：(bs, 256, h, w)
concat_feat：(bs, 256, h, w)
hyper_final：(bs, 64, h, w)
corr_query_mask：(bs, 1, h, w)
gam：(bs, 256, h, w)
拼接后：

merge_feat：(bs, 833, h, w)
处理后：

merge_feat：(bs, 256, h, w)

supp_pro 是通过对多个支持图像的特征图进行加权平均得到的。具体来说，supp_pro 的初始尺寸为 (bs, 256, shot, 1)


query_backbone_layers[2] 通过双线性插值上采样到与 query_backbone_layers[3] 相同的尺寸 (bs, 256, h/16, w/16)。
然后将 query_backbone_layers[3] 和上采样后的 query_backbone_layers[2] 在通道维度上拼接，得到 query_feat，其维度为 (bs, 512 + 256, h/16, w/16)，即 (bs, 768, h/16, w/16)。
```
query_feat：查询图像的特征图。
concat_feat：支持图像的特征图，通过 supp_pro 扩展到与 query_feat 相同的尺寸。
hyper_final：超特征图，通过 hyper_final 模块处理得到。
corr_query_mask：查询图像与支持图像之间的相关性特征图。
gam：通过 Attention 模块处理得到的特征图。

## 多相似性模块

1. **输入特征提取**：输入由查询图像 $I_q$和支持图像 $I_s$构成的图像对到预训练的骨干网络。该骨干网络冻结在训练过程中以实现对未见类别的泛化。骨干网络的最后三个块的特征图被提取并用于计算视觉对应关系：查询图像的特征图记为 $F_Q^$，支持图像的特征图记为 $F_S^$。
2. **支持特征掩码处理**：每个支持特征图 $F_s$通过双线性插值掩码 $M_s$进行掩码，以抑制背景区域。掩码后的支持特征图仅保留前景部分，使查询图像的特征只与支持图像的前景相关联。
3. **特征压缩**：为降低计算成本并避免过拟合，进一步压缩掩码后的支持特征图 $F_m^s$，将其维度从 $ℝ^{C_b \times H_\epsilon W_\epsilon}$减少至 $ℝ^{C_b \times N}$，其中 $N \ll H_\epsilon W_\epsilon$。
4. **视觉对应关系计算**：利用压缩后的支持特征图和查询特征图逐像素计算余弦距离，生成视觉对应图 $CS(x_q, x_s)$，以表示查询特征与支持特征之间的对应关系。
5. **多层视觉对应**：重复该过程以计算不同层次的视觉对应关系，并将这些视觉对应图拼接后，通过 $1 \times 1$卷积处理，最终生成统一的视觉表示。

## Attention Module

1. **特征提取与拼接**：通过骨干网络提取支持图像和查询图像的中间特征图，分别对应第2块和第3块特征图。将这些特征拼接后，通过1×1卷积层降维。
2. **注意力向量计算**：使用池化操作（P）、卷积网络（CN）、激活函数（σ）对拼接后的支持特征和支持掩码计算出注意力向量，以突出目标区域。
3. **生成注意力特征图**：利用注意力向量生成类别代表性注意力特征图，通过公式 $A_s = F_{s23} \odot V_a$实现。
4. **特征增强与分类**：将多层相似度、注意力特征、先验掩码、原型向量和查询特征拼接，经过ASPP模块的扩张卷积增强后，再通过卷积块和分类器生成最终预测掩码。
5. **训练损失**：采用二元交叉熵（BCE）损失，计算预测掩码与真实掩码间的差异。同时结合基础学习器损失和集成模块损失进行端到端训练。
6. **K-shot 分割**：对于K-shot（K > 1）场景，执行K次前向传播计算相似度、注意力特征等，并对这些特征逐层平均后输入ASPP模块，最后使用调整因子和全连接层进一步处理。

![](Paper/2024年10-12/attachments/Pasted%20image%2020241025162315.png)
