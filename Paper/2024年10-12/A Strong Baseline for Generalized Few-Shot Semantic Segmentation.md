---
title: A Strong Baseline for Generalized Few-Shot Semantic Segmentation
tags:
  - Segmentation
  - FSS
  - Baseline
  - Loss
date created: 2024-09-28
date modified: 2024-10-11
NotionID-Paper: 110e05f0-ba91-814f-9d05-f36f35ea2fbe
link-Paper: https://hikari-note.notion.site/A-Strong-Baseline-for-Generalized-Few-Shot-Semanti-110e05f0ba91814f9d05f36f35ea2fbe
---

首先，现有关于 FSS 的文献假设支持样本包含查询图像中的类别，这可能导致昂贵的手动选择过程。其次，尽管取得了显著成就，所有这些方法都专注于尽可能利用支持样本提取有效的目标信息，但忽略了在已知类别上的性能保持。此外，尽管在许多实际应用中新类别的数量不受限制，但大多数 FSS 方法设计为二元分类，这在面对多个新颖类别时并不理想。

GFSS 放宽了支持类别与查询类别必须相同的严格假设。这意味着，在这种新的学习范式下，无需提供包含与查询图像相同目标类别的支持图像。此外，此设置中的评估不仅涉及新类别，还包括基础类别，从而提供了一个更为现实的场景。

## 解决方案

另一个局限性在于当前方法的训练和测试阶段之间紧密纠缠，这通常限制了它们在测试时处理任意任务的能力。具体而言，现有的基于元学习的方法被设计用于处理二值分割[14]，因此需要相应地修改以应对多类别情况。尽管我们通过技术手段解决了这一问题，即使用多次前向传递（每类一次）并随后进行某种启发式分割图聚合，但这种方法扩展性差且缺乏原则性。

### 完全模块化的推理过程

我们提出了一种新的 GFSS 框架，即 DIaM（蒸馏信息最大化）。我们的方法受到著名的 InfoMax 原则的启发，该原则旨在最大化学习到的特征表示与其相应预测之间的互信息。为了在不要求显式监督的情况下减少对基础类别性能的下降，我们引入了一个 Kullback-Leibler 项，以确保旧模型和新模型对基础类别的预测一致性。

尽管在改进先前实验协议的实用性方面处于劣势，我们仍证明 DIaM 在现有的 GFSS 基准测试中优于当前最先进的方法，尤其在新型类别的分割方面表现突出。

基于我们的观察，我们超越了标准基准，提出了一种更具挑战性的情景，其中基础类和新类的数量相同。在此设定下，我们的方法与当前 GFSS SOTA 之间的差距进一步扩大，突显了现代 GFSS SOTA 处理大量新类别的不足，并强调了开发更模块化/可扩展方法的必要性。

### 迈向完全实用的环境

1. **解决训练过程中新类别的出现**：
    - 之前的研究所采用的方法是在训练阶段显式地移除包含新类别的图像，这需要事先知道这些新类别及其在特定图像中是否出现，这种做法不切实际。
    - 新方法允许在训练过程中保留这些包含新类别的图像，并将新类别的对象标记为背景，这样虽然可能因类别模糊而影响模型在测试时的表现，但更符合实际情况。
2. **放宽测试时标注要求**：
    - 原有的方法要求对支持集中来自基类的对象进行明确标注，这在实际应用中会增加大量的工作量。
    - 提出的新方法取消了这一要求，仅要求在测试时对新类别进行标注，基类及其他对象可以标记为背景，从而减少人力和财务成本。
3. **推理的模块化**：
    - 当前的少样本学习方法往往依赖于特定的模型架构或训练过程，不够灵活。
    - 新方法倡导开发模块化的推理方式，使其能够适应任何现有的模型，不需要专门的定制或从头开始的训练，降低了使用少样本学习技术的门槛，有利于其广泛采纳。

### InfoMax 主张最大化网络输入与输出之间的互信息

边际熵（marginal entropy）在模型预测中起到了确保不同类别之间分配公平的作用。具体来说，边际熵通过鼓励模型在各个类别之间进行均匀的像素分配，防止模型仅仅将所有像素分配给一个类别。

这种方法能够有效地应对类别不平衡的问题，因为在语义分割任务中，像素的分布往往是不均衡的，边际熵的使用可以帮助模型在不同类别之间实现更好的平衡。

在深度学习领域，InfoMax原则主要涉及最大化网络输入与输出之间的互信息。这种原则最初是在信息理论背景下提出的，目的是提升模型的表示能力和泛化性能。具体来说，InfoMax通过以下两个熵的差值进行优化：

1. **边缘熵 (Marginal Entropy, H(P))** ：这是对模型预测分布的总体熵进行的衡量。通过最大化边缘熵，模型被鼓励生成一个均衡的输出分布，这意味着不同类别之间的预测是均衡的，不会过度偏向某一类别。
2. **条件熵 (Conditional Entropy, H(P|X))** ：这是针对给定输入时，模型输出的不确定性。在优化中，条件熵被最小化，从而模型对于每个特定输入做出更确定（即自信）的预测。

在特定任务中，InfoMax被用来确保模型不仅对输入单个样本的预测是自信的，而且在整体上对所有样本有一个平衡且合理的类别分布。在无监督学习或聚类问题中，InfoMax原则可以引导模型在没有显式监督信号的情况下形成有意义的类别划分。

在该文献中，研究人员开始偏离传统的InfoMax，融入更多问题特定的元素和归纳偏置，以便更好地解决他们所研究的具体任务。

# 公式解析

### 互信息

GFSS 方法被赋予一个包含每个新类别若干图像的支持集，并应能在所有查询图像中预测所有可能的基础类别和新类别。与标准 FSS 方法相比，这种方式下模型对查询图像中存在的新类别一无所知。

![image.png](Paper/attachments/A%20Strong%20Baseline%20for%20Generalized%20Few-Shot%20Semanti%20105e05f0ba91805eac0bd16bc79279d8/image.png)

X和P分别是与像素分布和模型预测相关的随机变量

### 条件熵

![image.png](Paper/attachments/A%20Strong%20Baseline%20for%20Generalized%20Few-Shot%20Semanti%20105e05f0ba91805eac0bd16bc79279d8/image%201.png)

![image.png](Paper/attachments/A%20Strong%20Baseline%20for%20Generalized%20Few-Shot%20Semanti%20105e05f0ba91805eac0bd16bc79279d8/image%202.png)

[[2211.14126] 广义少样本语义分割的强基线 --- [2211.14126] A Strong Baseline for Generalized Few-Shot Semantic Segmentation](file:///E:/Paper/[2211.14126]%20%E5%B9%BF%E4%B9%89%E5%B0%91%E6%A0%B7%E6%9C%AC%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E5%BC%BA%E5%9F%BA%E7%BA%BF%20---%20[2211.14126]%20A%20Strong%20Baseline%20for%20Generalized%20Few-Shot%20Semantic%20Segmentation.html)

在论文中提到的$\max I(X; P)$函数是基于互信息（Mutual Information，MI）的概念来构建的，用于衡量模型的输入$X$（即图像的像素分布）和输出$P$（即模型的预测）之间的相互依赖性。互信息可以被理解为输出$P$中有多少信息是可以由输入$X$预测的。互信息的计算公式如下：

$$I(X; P) = H(P) - H(P | X)$$

这里，$H(P)$是边际熵（Marginal Entropy），而$H(P | X)$是条件熵（Conditional Entropy）。两者的计算方法如下：

1. **边际熵$H(P)$**：
   边际熵表示模型输出$P$的整体分布的熵，它衡量模型输出的分散程度。在语义分割的上下文中，$P$通常指的是所有像素预测类别的概率分布。边际熵的计算公式为：$$H(P) = -\sum_{c} p(c) \log p(c)$$这里$p(c)$是模型对于类别$c$的预测的边缘概率，即模型预测图像中某个像素属于类别$c$的概率。

2. **条件熵$H(P | X)$**：
   条件熵表示在已知输入$X$的情况下，输出$P$的熵。它衡量了模型输出的不确定性。在训练过程中，我们希望模型在给定输入$X$后能做出尽可能确定的预测，即条件熵尽可能低。条件熵的计算公式为：
$$H(P | X) = -\sum_{x} p(x) \sum_{c} p(c | x) \log p(c | x)$$
这里$p(c | x)$是在给定输入$x$的条件下，模型预测像素属于类别$c$的条件概率。

在训练过程中，通过最大化互信息$I(X; P)$，相当于同时最小化条件熵$H(P | X)$并适当调整边际熵$H(P)$。这使得模型在给定输入的情况下能够做出更有信心的预测（低条件熵），同时保证输出的整体分布不过于集中或偏离（适当的边际熵）。

论文中的方法通过优化这个基于互信息的目标函数，鼓励模型学习到的特征表示尽可能地保留类别预测的有用信息，从而提高模型在少样本语义分割任务中的性能。

#### 交叉熵

**交叉熵**（Cross-Entropy）是另一种熵的度量，它衡量的是两个概率分布之间的差异。在机器学习和深度学习中，交叉熵损失函数常用于分类任务，特别是在目标概率分布是one-hot编码的情况下。交叉熵的计算公式如下：
$$ H(p, q) = -\sum_{i} p(i) \log q(i) $$
交叉熵损失函数的核心思想是最小化模型预测概率分布和真实概率分布之间的差异。通过最小化交叉熵损失，我们可以训练模型使其输出的概率分布尽可能接近真实的标签分布。

### **GFSS差异**

1. **任务定义**
    - **FSS（少样本语义分割）** ：通常关注在给定少量标注样本的情况下，对新类别进行分割。任务中通常假设测试时只出现新类别。
    - **GFSS（广义少样本语义分割）** ：不仅要处理新类别，还要在测试时同时处理已知类别和新类别。即需要在同一任务中区分新旧类别。
2. **支持集和查询集**
    - **FSS**：支持集包含少量标注样本，通常只针对新类别。查询集中只出现这些新类别。
    - **GFSS**：支持集同样包含新类别的少量样本，但查询集中既可能出现新类别，也可能出现已知类别，模型需同时识别这两类。
3. **新类别处理**
    - **FSS**：模型专注于适应新类别，可能忽视已知类别的区分。
    - **GFSS**：模型需要同时考虑新类别和已知类别的区分，增加了任务复杂性。
4. **新类信息的包含**
    - **FSS**：模型在训练时通常不包含新类信息，主要依赖支持集中提供的少量样本进行新类别学习。
    - **GFSS**：模型需在训练中考虑如何有效集成新类信息，同时保持对已知类别的识别能力。
5. **模型信息包含**
    - **FSS模型** ：通常在训练过程中并不直接包含新类信息，而是通过元学习等方法学习到适应新类别的能力。
    - **GFSS模型** ：需要设计机制在训练中保留旧类别的特征，同时能够有效适应和区分新类别。这可能涉及到特征对齐、类别关系建模等技术。

# KIMI总结

这篇论文的标题是《A Strong Baseline for Generalized Few-Shot Semantic Segmentation》，由Sina Hajimiri、Malik Boudiaf、Ismail Ben Ayed和Jose Dolz等人撰写。这篇论文提出了一个用于广义小样本语义分割（Generalized Few-Shot Semantic Segmentation，GFSS）的强基线框架。下面我将总结这篇论文的主要内容，特别关注方法（Method）部分。

**摘要（Abstract）**
论文介绍了一个通用的少样本分割框架，其训练过程简单，推理阶段易于优化。作者提出了一个基于InfoMax原则的简单而有效的模型，该模型通过最大化学习到的特征表示与其对应预测之间的互信息（Mutual Information，MI）来工作。此外，该模型通过知识蒸馏项保留了基础类别的知识。所提出的推理方法在流行的小样本分割基准测试中取得了显著的改进。

**引言（Introduction）**
论文讨论了深度学习在图像内容自动解析和语义理解方面取得的进展，指出标准分割方法通常需要大量的训练数据，限制了对新类别的扩展性。作者提出了广义小样本语义分割（GFSS）的概念，放宽了支持类别和查询类别必须相同的严格假设。

**方法（Our Method）**
方法部分是论文的核心，作者提出了一个新的GFSS框架，称为DIaM（Distilled Information Maximization）。以下是该方法的要点：

1. **训练（Training）**：作者将分割模型分为特征提取器和线性分类器，使用标准监督学习训练基础类别。
2. **推理（Inference）**：在测试阶段，作者冻结了特征提取器，并为新类别增加了新的原型到预训练的分类器中。然后，他们提出了一个基于互信息的优化目标函数。
3. **高置信度预测（Enforcing high-confidence）**：为了确保每个像素的高置信度预测，作者引入了交叉熵算子，并利用支持图像的真实标签作为监督信号。
4. **解决类别不平衡（Addressing class imbalance）**：为了确保不同类别间公平的分配，作者引入了边际熵，并考虑使用模型初始边际分布估计的先验。
5. **保留基础知识（Preserving base knowledge）**：为了防止在适应新类别时对基础类别性能的损害，作者提出了一个知识蒸馏项，鼓励模型在基础类别上的预测保持接近其旧预测。

**实验（Experiments）**
作者在PASCAL-5𝑖和COCO-20𝑖这两个流行的小样本分割基准上评估了他们的方法，并与现有的GFSS方法进行了比较。实验结果显示，DIaM在新类别的分割上取得了显著的性能提升，同时保持了对基础类别的高性能。

**结论（Conclusion）**
作者提出了一种新的广义小样本分割方法，该方法具有标准监督训练方案和轻量级推理阶段，可以应用于任何学习到的特征提取器和分类器。与以往的工作相比，所提出的方法在新类别的学习上取得了显著的改进，同时保持了基础类别的高性能。

整体而言，这篇论文的主要贡献在于提出了一个新的小样本语义分割框架，该框架通过一个简单有效的模型和优化的推理过程，在少样本的情况下实现了对新类别的鲁棒分割。

# Method

1. 强化高置信度(条件熵)
2. 解决类别不平衡问题(边缘熵)
3. 保留基础知识(知识蒸馏和新旧映射)

## 保留基础知识

### **蒸馏损失 (Distillation Loss)**

- **功能**: 使用分类器在训练时保存的快照作为“教师模型”（teacher），对当前的“学生模型”（student）进行知识蒸馏，计算两者的 KL 散度作为蒸馏损失。
- **概念**: 知识蒸馏（Knowledge Distillation）是一种通过让一个较小的学生模型模仿一个较大的教师模型来传递知识的技术。KL 散度用于衡量两者输出分布之间的差异。

在你提供的公式和描述中，新旧映射涉及将**当前模型的预测概率**与**旧模型的预测概率**进行比较，并通过对两者进行映射，使它们在相同的标签空间上进行比较，从而实现知识蒸馏。这涉及到增量学习的背景，模型需要在**保留旧类知识**的同时**学习新类知识**。

### 新旧映射的动机

因为基础分类器（旧模型）的预测概率空间与当前模型的预测概率空间不同（当前模型包含新类别），需要把它们映射到**相同的标签空间**，以便进行一致性对比。旧模型的预测只包含基础类别（`C_b`），而新模型的预测包含基础类别和新类别（`C_b + C_n`）。因此，需要将新模型的预测“投影”到旧类别上，进行对齐比较。

### 新旧映射的具体处理

为了把新模型的预测映射到旧模型的标签空间，公式 (11) 提供了一种映射操作：

$$
\pi_{\text{new2old}}(\mathbf{p}) (j) = [p_0 + \sum_{i=1}^{|C_n|} p_{|C_b|+i}, p_1, p_2, \dots, p_{|C_b|}]^\top
$$

解释如下：
- **$\mathbf{p}$** 是新模型的预测概率分布，包含了基础类别$C_b$和新类别$C_n$的预测。
- **$p_0$** 是背景类的概率，表示模型预测的背景或“其他”。
- **$\sum_{i=1}^{|C_n|} p_{|C_b|+i}$** 是所有新类预测的概率和。这个部分的处理是因为在旧模型的标签空间中，这些新类之前是未定义的，它们在旧模型中的角色类似于“背景”类。
- 其他项 **$p_1, p_2, \dots, p_{|C_b|}$** 则保持不变，对应的是基础类别的预测。

这个映射过程可以被看作是把新模型的预测**压缩回旧模型的标签空间**，新模型的“新类”被折叠成旧模型的背景类，并保持了基础类别的概率不变。最终，新模型的预测被转化为与旧模型可比较的形式。

### 为什么要做这样的映射？

1. **标签空间对齐**：新模型学会了新类别后，标签空间扩大了。如果直接将新模型的预测与旧模型的预测进行对比，结果可能不合理。因此，需要旧模型的标签空间，使得两者能够在同一个类别集合上进行比较。这种对齐通过将新模型的“新类别”预测归入旧模型的“背景”类来实现，确保旧类别预测的一致性。
2. **保留基础知识**：新模型在学习新类别的同时，可能会遗忘旧类别的知识。这种现象被称为**灾难性遗忘**。为了防止新模型在基础类别上的表现下降，公式 (12) 中引入了一个知识蒸馏项：
  $$
   \mathcal{L}_{KD} = KL(\pi_{\text{new2old}}(\mathbf{p}_{|\mathbb{S}|+1}) \, || \, \mathbf{p}_{|\mathbb{S}|+1}^{\text{old}})
  $$
   这里使用了**KL散度**来衡量当前模型和旧模型在基础类别上的预测分布差异。通过最小化这个项，模型被鼓励在基础类别的预测上保持与旧模型一致，确保模型保留已经学习的基础知识。

3. **新旧类别的协同学习**：通过这种映射，新模型不仅能够学习新类别，还能避免对基础类别的预测产生过大的偏移，从而实现对新旧类别的协同学习。这对于增量学习场景特别重要，因为基础类别和新类别的训练数据量存在不对称性（基础类别有更多的数据进行训练，而新类别的数据较少）。

### 总结

- **新旧映射**的主要作用是将新模型的预测概率映射回旧模型的标签空间，这样可以与旧模型的预测进行对齐比较。通过将新类别的预测概率加到背景类上，确保基础类别的概率不变，并实现一致性的比较。
- **知识蒸馏项**通过最小化新模型和旧模型在基础类别上的预测差异，保留了旧模型的基础知识，避免了灾难性遗忘现象。
- 这种方法有助于新模型在学习新类别的同时，保持对基础类别的正确预测，从而达到新旧知识的平衡。

## KL 散度（Kullback-Leibler Divergence，KL Divergence）

是一种用于衡量两个概率分布之间差异的统计量。在信息论和概率论中，KL 散度用于量化一个分布 $P$ 在多大程度上与另一个参考分布 $Q$ 不同。

### 公式

对于离散概率分布 $P$ 和 $Q$，KL 散度的定义如下：

$$
D_{\text{KL}}(P \parallel Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
$$

对于连续分布，KL 散度为：

$$
D_{\text{KL}}(P \parallel Q) = \int_{-\infty}^{\infty} P(x) \log \frac{P(x)}{Q(x)} dx
$$

其中：
- $P(x)$ 是目标分布（通常是“真实”分布）
- $Q(x)$ 是参考分布（通常是“近似”分布）
- 该散度并不对称，即 $D_{\text{KL}}(P \parallel Q) \neq D_{\text{KL}}(Q \parallel P)$

### 直观解释

KL 散度可以看作是衡量“当我们使用分布 $Q$ 来近似分布 $P$ 时，会损失多少信息”。KL 散度值越大，表示两个分布之间的差异越大，说明用 $Q$ 来近似 $P$ 时的“误差”越大。相反，如果 $P$ 和 $Q$ 非常接近，KL 散度接近 0。

### 深度学习中的应用

在深度学习中，KL 散度主要用于以下场景：
1. **知识蒸馏（Distillation）**：在知识蒸馏中，较小的“学生模型”从较大的“教师模型”中学习。在这种情况下，KL 散度用于衡量学生模型输出的概率分布与教师模型输出的概率分布之间的差异，进而引导学生模型的学习。
2. **变分自编码器（VAE）**：KL 散度用于衡量近似后验分布与先验分布之间的差异，以最小化模型生成的潜在变量的分布与真实分布之间的差异。
3. **分类问题中的正则化项**：有时在分类模型中，KL 散度用于使模型的输出分布更接近某个期望分布。

### 在代码中的使用

在你提供的代码中，`distillation_loss` 函数就是使用 KL 散度来计算当前模型预测的概率分布（`adjusted_curr_p`）与之前的“快照”模型的概率分布（`snapshot_p`）之间的差异。这是在优化过程中的一种正则化，用来确保模型在新的任务上学习时，不会偏离它在基础任务上的表现。

## 边际熵 $H(P)$

是衡量一个随机变量的概率分布 $P$ 的不确定性程度的指标。它反映了分布的**均匀性**：即分布越均匀，熵值越大；反之，分布越集中于某些特定的值，熵值越小。具体来说，边际熵的公式为：
$$
H(P) = - \sum_{i} p_i \log p_i
$$
其中 $p_i$ 表示概率分布 $P$ 中第 $i$ 个事件发生的概率。熵是**概率分布的性质**，它与分布的形状紧密相关：

1. **均匀分布的熵最大**：
   当概率分布 $P$ 是均匀分布时，即所有可能的事件都有相同的概率（例如，投掷一个公平的骰子），熵达到最大值。对于 $n$ 个事件的均匀分布，每个事件的概率都是 $\frac{1}{n}$，此时的熵为：
$$
   H(P) = - \sum_{i=1}^{n} \frac{1}{n} \log \frac{1}{n} = \log n
$$

   这种情况下，熵最大，因为不确定性最大——每个事件都有相同的可能性发生，系统的信息量最高。

2. **单峰分布的熵较小**：
   如果概率分布倾向于某些特定事件，即分布集中在少数事件上，而其他事件的概率接近于零（例如，投掷一个不公平的骰子），则熵会较小。因为当某些事件的概率大大超过其他事件时，系统的不确定性降低，信息量也随之减少。

3. **确定分布的熵最小（为零）**：
   当概率分布完全确定，即某个事件的概率为 1，而其他事件的概率全为 0 时，熵达到最小值 $H(P) = 0$。这是因为在这种情况下没有任何不确定性——我们可以确定某个事件一定会发生。

### 熵和概率分布的相关性总结

- 熵越大，意味着事件的概率分布越均匀，系统的**不确定性越大**。
- 熵越小，意味着分布越集中于某些事件，系统的**不确定性越小**。
- 当分布是确定的（只有一个事件发生），熵为零，因为系统**没有不确定性**。

因此，边际熵 $H(P)$ 可以看作是对分布均匀性和不确定性的量化。

