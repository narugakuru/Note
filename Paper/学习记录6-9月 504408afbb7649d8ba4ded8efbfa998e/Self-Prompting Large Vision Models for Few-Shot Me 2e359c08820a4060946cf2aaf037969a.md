---
tags:
  - FSS
  - Data-Enhancement
---

# Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation

Owner: hikari

这篇论文的标题是《Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation》，作者是Qi Wu、Yuyao Zhang和Marawan Elbatel，来自香港科技大学和西班牙赫罗纳大学的计算机视觉与机器人研究所。论文主要研究了如何利用大型视觉模型进行少量样本（Few-Shot）医学图像分割。

**摘要与引言：**

- 论文指出，大型基础模型（如Segment Anything Model, SAM）在医学图像分割领域展现出巨大潜力，但现有方法依赖于大量数据或特定任务的先验提示。
- 作者提出了一种新颖的自提示（self-prompting）方法，通过简单的线性像素级分类器，利用SAM的嵌入空间来自我提示，以少量数据样本实现竞争性能。

**方法论（重点总结）：**

- **训练数据集**：由图像和分割真值对组成，目标是设计一个即插即用的自提示单元，仅使用少量标记数据（如k张图像）来提供分割目标的位置和大小信息。
- **自提示单元**：通过粗略掩码作为参考，利用SAM的强大编码器（Vision Transformer, ViT）将输入图像编码为向量，然后自提示单元接收图像嵌入来提供边界框和点作为提示。
- **位置点**：使用距离变换找到预测掩码内的一点来代表位置。
- **边界框**：通过线性像素级分类器生成的预测掩码的最小和最大X、Y坐标来生成，并添加0-20像素的扰动。
- **训练目标**：使用逻辑回归对每个像素进行分类，损失函数为像素级的交叉熵损失。

**实验：**

- 使用Kvasir-SEG和ISIC-2018数据集进行实验，采用5折交叉验证。
- 实现细节：使用ViT-B版本的SAM作为骨干网络，训练像素级分类器，并使用scikit-learn库中的逻辑回归模块。
- 结果：使用Dice和IoU作为评估指标，与MedSAM和SAMed等其他微调模型进行比较，结果表明作者的方法在少量样本设置下优于其他方法。

**结论：**

- 论文展示了利用自提示和大规模视觉基础模型进行医学图像分割的潜力和可行性。
- 提出的方法比传统的少量样本学习模型更用户友好，且所需的数据量远少于其他SAM微调模型。
- 未来的研究可以更注重从SAM的输出中获得更准确的提示，也可以探索将自提示与其他微调方法结合。

**补充材料：**

- 讨论了方法的局限性，如在处理多实例分割任务时的性能限制，以及在不同模态数据上的测试结果。
- 进行了消融研究，探讨了不同数量的训练样本和不同的提示方法对性能的影响。

整体而言，这篇论文提出了一种创新的方法，通过自提示机制在少量样本的情况下提高医学图像分割的性能，这对于数据稀缺的医学图像分析领域具有重要意义。