<<<<<<< HEAD
---
tags:
  - FSS
  - Loss
  - 框架
---

=======
>>>>>>> 516cbd4495f625d0ca6b3b21239e3652645bffc2
# A Strong Baseline for Generalized Few-Shot Semantic Segmentation

Owner: hikari

首先，现有关于 FSS 的文献假设支持样本包含查询图像中的类别，这可能导致昂贵的手动选择过程。其次，尽管取得了显著成就，所有这些方法都专注于尽可能利用支持样本提取有效的目标信息，但忽略了在已知类别上的性能保持。此外，尽管在许多实际应用中新类别的数量不受限制，但大多数 FSS 方法设计为二元分类，这在面对多个新颖类别时并不理想。

GFSS 放宽了支持类别与查询类别必须相同的严格假设。这意味着，在这种新的学习范式下，无需提供包含与查询图像相同目标类别的支持图像。此外，此设置中的评估不仅涉及新类别，还包括基础类别，从而提供了一个更为现实的场景。

## 解决方案

另一个局限性在于当前方法的训练和测试阶段之间紧密纠缠，这通常限制了它们在测试时处理任意任务的能力。具体而言，现有的基于元学习的方法被设计用于处理二值分割[14]，因此需要相应地修改以应对多类别情况。尽管我们通过技术手段解决了这一问题，即使用多次前向传递（每类一次）并随后进行某种启发式分割图聚合，但这种方法扩展性差且缺乏原则性。

### 完全模块化的推理过程

我们提出了一种新的 GFSS 框架，即 DIaM（蒸馏信息最大化）。我们的方法受到著名的 InfoMax 原则的启发，该原则旨在最大化学习到的特征表示与其相应预测之间的互信息。为了在不要求显式监督的情况下减少对基础类别性能的下降，我们引入了一个 Kullback-Leibler 项，以确保旧模型和新模型对基础类别的预测一致性。

尽管在改进先前实验协议的实用性方面处于劣势，我们仍证明 DIaM 在现有的 GFSS 基准测试中优于当前最先进的方法，尤其在新型类别的分割方面表现突出。

基于我们的观察，我们超越了标准基准，提出了一种更具挑战性的情景，其中基础类和新类的数量相同。在此设定下，我们的方法与当前 GFSS SOTA 之间的差距进一步扩大，突显了现代 GFSS SOTA 处理大量新类别的不足，并强调了开发更模块化/可扩展方法的必要性。

### 迈向完全实用的环境

1. **解决训练过程中新类别的出现**：
    - 之前的研究所采用的方法是在训练阶段显式地移除包含新类别的图像，这需要事先知道这些新类别及其在特定图像中是否出现，这种做法不切实际。
    - 新方法允许在训练过程中保留这些包含新类别的图像，并将新类别的对象标记为背景，这样虽然可能因类别模糊而影响模型在测试时的表现，但更符合实际情况。
2. **放宽测试时标注要求**：
    - 原有的方法要求对支持集中来自基类的对象进行明确标注，这在实际应用中会增加大量的工作量。
    - 提出的新方法取消了这一要求，仅要求在测试时对新类别进行标注，基类及其他对象可以标记为背景，从而减少人力和财务成本。
3. **推理的模块化**：
    - 当前的少样本学习方法往往依赖于特定的模型架构或训练过程，不够灵活。
    - 新方法倡导开发模块化的推理方式，使其能够适应任何现有的模型，不需要专门的定制或从头开始的训练，降低了使用少样本学习技术的门槛，有利于其广泛采纳。

### InfoMax 主张最大化网络输入与输出之间的互信息

边际熵（marginal entropy）在模型预测中起到了确保不同类别之间分配公平的作用。具体来说，边际熵通过鼓励模型在各个类别之间进行均匀的像素分配，防止模型仅仅将所有像素分配给一个类别。

这种方法能够有效地应对类别不平衡的问题，因为在语义分割任务中，像素的分布往往是不均衡的，边际熵的使用可以帮助模型在不同类别之间实现更好的平衡。

在深度学习领域，InfoMax原则主要涉及最大化网络输入与输出之间的互信息。这种原则最初是在信息理论背景下提出的，目的是提升模型的表示能力和泛化性能。具体来说，InfoMax通过以下两个熵的差值进行优化：

1. **边缘熵 (Marginal Entropy, H(P))** ：这是对模型预测分布的总体熵进行的衡量。通过最大化边缘熵，模型被鼓励生成一个均衡的输出分布，这意味着不同类别之间的预测是均衡的，不会过度偏向某一类别。
2. **条件熵 (Conditional Entropy, H(P|X))** ：这是针对给定输入时，模型输出的不确定性。在优化中，条件熵被最小化，从而模型对于每个特定输入做出更确定（即自信）的预测。

在特定任务中，InfoMax被用来确保模型不仅对输入单个样本的预测是自信的，而且在整体上对所有样本有一个平衡且合理的类别分布。在无监督学习或聚类问题中，InfoMax原则可以引导模型在没有显式监督信号的情况下形成有意义的类别划分。

在该文献中，研究人员开始偏离传统的InfoMax，融入更多问题特定的元素和归纳偏置，以便更好地解决他们所研究的具体任务。

## 公式

### 互信息

GFSS 方法被赋予一个包含每个新类别若干图像的支持集，并应能在所有查询图像中预测所有可能的基础类别和新类别。与标准 FSS 方法相比，这种方式下模型对查询图像中存在的新类别一无所知。

![image.png](Paper/attachments/A%20Strong%20Baseline%20for%20Generalized%20Few-Shot%20Semanti%20105e05f0ba91805eac0bd16bc79279d8/image.png)

X和P分别是与像素分布和模型预测相关的随机变量

### 条件熵

![image.png](Paper/attachments/A%20Strong%20Baseline%20for%20Generalized%20Few-Shot%20Semanti%20105e05f0ba91805eac0bd16bc79279d8/image%201.png)

![image.png](Paper/attachments/A%20Strong%20Baseline%20for%20Generalized%20Few-Shot%20Semanti%20105e05f0ba91805eac0bd16bc79279d8/image%202.png)

在论文中提到的 \( \max I(X; P) \) 函数是基于互信息（Mutual Information，MI）的概念来构建的，用于衡量模型的输入 \( X \)（即图像的像素分布）和输出 \( P \)（即模型的预测）之间的相互依赖性。互信息可以被理解为输出 \( P \) 中有多少信息是可以由输入 \( X \) 预测的。互信息的计算公式如下：

\[ I(X; P) = H(P) - H(P | X) \]

这里，\( H(P) \) 是边际熵（Marginal Entropy），而 \( H(P | X) \) 是条件熵（Conditional Entropy）。两者的计算方法如下：

1. **边际熵 \( H(P) \)**：
边际熵表示模型输出 \( P \) 的整体分布的熵，它衡量模型输出的分散程度。在语义分割的上下文中，\( P \) 通常指的是所有像素预测类别的概率分布。边际熵的计算公式为：
    
    \[ H(P) = -\sum_{c} p(c) \log p(c) \]
    
    这里 \( p(c) \) 是模型对于类别 \( c \) 的预测的边缘概率，即模型预测图像中某个像素属于类别 \( c \) 的概率。
    
2. **条件熵 \( H(P | X) \)**：
条件熵表示在已知输入 \( X \) 的情况下，输出 \( P \) 的熵。它衡量了模型输出的不确定性。在训练过程中，我们希望模型在给定输入 \( X \) 后能做出尽可能确定的预测，即条件熵尽可能低。条件熵的计算公式为：
    
    \[ H(P | X) = -\sum_{x} p(x) \sum_{c} p(c | x) \log p(c | x) \]
    
    这里 \( p(c | x) \) 是在给定输入 \( x \) 的条件下，模型预测像素属于类别 \( c \) 的条件概率。
    

在训练过程中，通过最大化互信息 \( I(X; P) \)，相当于同时最小化条件熵 \( H(P | X) \) 并适当调整边际熵 \( H(P) \)。这使得模型在给定输入的情况下能够做出更有信心的预测（低条件熵），同时保证输出的整体分布不过于集中或偏离（适当的边际熵）。

论文中的方法通过优化这个基于互信息的目标函数，鼓励模型学习到的特征表示尽可能地保留类别预测的有用信息，从而提高模型在少样本语义分割任务中的性能。

### **GFSS差异**

1. **任务定义**
    - **FSS（少样本语义分割）** ：通常关注在给定少量标注样本的情况下，对新类别进行分割。任务中通常假设测试时只出现新类别。
    - **GFSS（广义少样本语义分割）** ：不仅要处理新类别，还要在测试时同时处理已知类别和新类别。即需要在同一任务中区分新旧类别。
2. **支持集和查询集**
    - **FSS**：支持集包含少量标注样本，通常只针对新类别。查询集中只出现这些新类别。
    - **GFSS**：支持集同样包含新类别的少量样本，但查询集中既可能出现新类别，也可能出现已知类别，模型需同时识别这两类。
3. **新类别处理**
    - **FSS**：模型专注于适应新类别，可能忽视已知类别的区分。
    - **GFSS**：模型需要同时考虑新类别和已知类别的区分，增加了任务复杂性。
4. **新类信息的包含**
    - **FSS**：模型在训练时通常不包含新类信息，主要依赖支持集中提供的少量样本进行新类别学习。
    - **GFSS**：模型需在训练中考虑如何有效集成新类信息，同时保持对已知类别的识别能力。
5. **模型信息包含**
    - **FSS模型** ：通常在训练过程中并不直接包含新类信息，而是通过元学习等方法学习到适应新类别的能力。
    - **GFSS模型** ：需要设计机制在训练中保留旧类别的特征，同时能够有效适应和区分新类别。这可能涉及到特征对齐、类别关系建模等技术。

# KIMI总结

这篇论文的标题是《A Strong Baseline for Generalized Few-Shot Semantic Segmentation》，由Sina Hajimiri、Malik Boudiaf、Ismail Ben Ayed和Jose Dolz等人撰写。这篇论文提出了一个用于广义小样本语义分割（Generalized Few-Shot Semantic Segmentation，GFSS）的强基线框架。下面我将总结这篇论文的主要内容，特别关注方法（Method）部分。

**摘要（Abstract）**
论文介绍了一个通用的少样本分割框架，其训练过程简单，推理阶段易于优化。作者提出了一个基于InfoMax原则的简单而有效的模型，该模型通过最大化学习到的特征表示与其对应预测之间的互信息（Mutual Information，MI）来工作。此外，该模型通过知识蒸馏项保留了基础类别的知识。所提出的推理方法在流行的小样本分割基准测试中取得了显著的改进。

**引言（Introduction）**
论文讨论了深度学习在图像内容自动解析和语义理解方面取得的进展，指出标准分割方法通常需要大量的训练数据，限制了对新类别的扩展性。作者提出了广义小样本语义分割（GFSS）的概念，放宽了支持类别和查询类别必须相同的严格假设。

**方法（Our Method）**
方法部分是论文的核心，作者提出了一个新的GFSS框架，称为DIaM（Distilled Information Maximization）。以下是该方法的要点：

1. **训练（Training）**：作者将分割模型分为特征提取器和线性分类器，使用标准监督学习训练基础类别。
2. **推理（Inference）**：在测试阶段，作者冻结了特征提取器，并为新类别增加了新的原型到预训练的分类器中。然后，他们提出了一个基于互信息的优化目标函数。
3. **高置信度预测（Enforcing high-confidence）**：为了确保每个像素的高置信度预测，作者引入了交叉熵算子，并利用支持图像的真实标签作为监督信号。
4. **解决类别不平衡（Addressing class imbalance）**：为了确保不同类别间公平的分配，作者引入了边际熵，并考虑使用模型初始边际分布估计的先验。
5. **保留基础知识（Preserving base knowledge）**：为了防止在适应新类别时对基础类别性能的损害，作者提出了一个知识蒸馏项，鼓励模型在基础类别上的预测保持接近其旧预测。

**实验（Experiments）**
作者在PASCAL-5𝑖和COCO-20𝑖这两个流行的小样本分割基准上评估了他们的方法，并与现有的GFSS方法进行了比较。实验结果显示，DIaM在新类别的分割上取得了显著的性能提升，同时保持了对基础类别的高性能。

**结论（Conclusion）**
作者提出了一种新的广义小样本分割方法，该方法具有标准监督训练方案和轻量级推理阶段，可以应用于任何学习到的特征提取器和分类器。与以往的工作相比，所提出的方法在新类别的学习上取得了显著的改进，同时保持了基础类别的高性能。

整体而言，这篇论文的主要贡献在于提出了一个新的小样本语义分割框架，该框架通过一个简单有效的模型和优化的推理过程，在少样本的情况下实现了对新类别的鲁棒分割。