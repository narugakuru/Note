# 考点																		作者--小岚岚

![image-20220105164700026](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/image-20220105164700026.png)

# 大数据概述



### 大数据的概念（填空）

- 数据量大
- 数据类型繁多
- 处理速度快
- 价值密度低



### 云计算3种服务模式（填空）

- 基础设施即服务
- 平台即服务
- 软件即服务



# 分布式文件系统HDFS

> HDFS默认一个块大小为64MB

### HDFS命名空间管理

- HDFS命名空间包括：目录，文件，块
- 指命名空间支持对HDFS中的目录，文件和块做雷士文件系统的**增删改**等基本操作。
- HDFS集群只有一个命名空间，并且只有唯一一个名称节点，该节点对命名空间进行管理。

### HDFS架构

HDFS包含两个层次：**命名空间管理**（Namespace） 和 **块/存储管理**（Block Storage）。

- 命名空间管理（Namespace）
  HDFS的命名空间包含目录、文件和块。命名空间管理是指命名空间支持对HDFS中的目录、文件和块做类似文件系统的创建、修改、删除、列表文件和目录等基本操作。
- 块/存储管理（Block Storage）
  在块存储服务中包含两部分工作：块管理和物理存储。这是一个更通用的存储服务。其他的应用可以直接建立在Block Storage上，如HBase，Foreign Namespaces等。
  - 块管理
    - 处理Data Node向Name Node注册的请求，处理Data Node的成员关系，处理来自Data Node周期性的心跳。
    - 处理来自块的报告信息，维护块的位置信息。
    - 处理与块相关的操作：块的创建、删除、修改及获取块信息。
    - 管理副本放置（replica placement）和块的复制及多余块的删除。
  - 物理存储
    所谓物理存储就是：Data Node把块存储到本地文件系统中，对本地文件系统的读、写。



### Federation关键技术点

- 命名空间管理
  Federation中存在多个命名空间，如何划分和管理这些命名空间非常关键。在Federation中并采用“文件名hash”的方法，因为该方法的locality非常差，比如：查看某个目录下面的文件，如果采用文件名hash的方法存放文件，则这些文件可能被放到不同namespace中，HDFS需要访问所有namespace，代价过大。为了方便管理多个命名空间，HDFS Federation采用了经典的Client Side Mount Table。
  ![client-side-mount-table](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/o_client-side-mount-table.jpg)
  如上图所示，下面四个深色三角形代表一个独立的命名空间，上方浅色的三角形代表从客户角度去访问的子命名空间。各个深色的命名空间Mount到浅色的表中，客户可以访问不同的挂载点来访问不同的命名空间，这就如同在Linux系统中访问不同挂载点一样。这就是HDFS Federation中命名空间管理的基本原理：将各个命名空间挂载到全局mount-table中，就可以做将数据到全局共享；同样的命名空间挂载到个人的mount-table中，这就成为应用程序可见的命名空间视图。
- Block Pool（块池）
  所谓Block pool(块池)就是属于单个命名空间的一组block(块)。每一个datanode为所有的block pool存储块。Datanode是一个物理概念，而block pool是一个重新将block划分的逻辑概念。同一个datanode中可以存着属于多个block pool的多个块。Block pool允许一个命名空间在不通知其他命名空间的情况下为一个新的block创建Block ID。同时，一个Namenode失效不会影响其下的datanode为其他Namenode的服务。



# 分布式数据库HBase

> HBase是BigTable的开源版本。是建立的HDFS之上，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统。
>
> BigTable具备以下特性：支持大规模海量数据，分布式并发数据处理效率极高，易于拓展且支持动态伸缩，适用于廉价设备，，适合读操作不适合写操作。

### HBase和BigTable底层技术对应关系

| 项目         | BigTable  | HBase            |
| ------------ | --------- | ---------------- |
| 文件存储系统 | GFS       | HDFS             |
| 海量数据处理 | MapReduce | Hadoop MapReduce |
| 协调服务管理 | Chubby    | Zookeeper        |



### HBase基本概念

**HBase中的表一般有这样的特点：**

- 大：一个表可以有上亿行，上百万列
- 面向列:面向列(族)的存储和权限控制，列(族)独立检索。
- 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。



与nosql数据库们一样,**row key是用来检索记录的主键**。

**访问hbase table中的行**，只有三种方式：

1. 通过单个row key访问
2. 通过row key的range
3. 全表扫描



**列族**

hbase表中的每个列，都归属与某个列族。列族是表的chema的一部分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如*courses:history ， courses:math 都属于 courses* 这个列族。

访问控制、磁盘和内存的使用统计都是**在列族层面进行**的。实际应用中，列族上的控制权限能 帮助我们管理不同类型的应用：我们允许一些应用可以添加新的基本数据、一些应用可以读取基本数据并创建继承的列族、一些应用则只允许浏览数据（甚至可能因 为隐私的原因不能浏览所有数据）。

 

**时间戳**

HBase中通过row和columns确定的为一个存贮单元称为cell。每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由hbase(在数据写入时自动 )赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。

为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase提供了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。

 

**Cell**

由*{row key, column(* =<family> + <label>*), version}* 唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮。



### HBase 的Key-Value

HBase本质上其实就是`Key-Value`的数据库，上一次我们学`Key-Value`数据库还是Redis呢。那在HBase里边，Key是什么？Value是什么？

我们看一下下面的HBase`Key-Value`结构图：



![img](https://pic4.zhimg.com/v2-5c2c05489dc0c4af823d26641dc0e477_r.jpg)



Key由RowKey(行键)+ColumnFamily（列族）+Column Qualifier（列修饰符）+TimeStamp（时间戳--版本）+KeyType（类型）组成，而Value就是实际上的值。

对比上面的例子，其实很好理解，因为我们修改一条数据其实上是在原来的基础上增加一个版本的，那我们要**准确定位**一条数据，那就得（RowKey+Column+时间戳）。

KeyType是什么？我们上面只说了「修改」的情况，你们有没有想过，如果要删除一条数据怎么做？实际上也是增加一条记录，只不过我们在KeyType里边设置为“Delete”就可以了。

### HBase三层架构

| 层次   | 名称          | 作用                                                         |
| ------ | ------------- | ------------------------------------------------------------ |
| 第一层 | Zookeeper文件 | 记录ROOT表的位置信息                                         |
| 第二层 | -ROOT-表      | 记录了META表的**Region**位置信息，ROOT表只能有一个Region。   |
| 第三层 | .META.表      | 记录了HBase所有用户数据表的**Region**位置信息，META表可以有多个Region。 |

- **每个Region会被限制为128MB。**

- 表中的每行映射大约占用**1kb的内存**，一个Region可保存2^17行数据。

- 三层结构可以保存的Region数目是**2^34个Region**。

- **每次寻址需访问3次内存**，为加速寻址通常会设立一个高速缓存空间。

  

### Region服务器

>Region是HBase和核心模块，而store是Region服务器的核心。
>
>每个store对应一个列族的存储。

#### HRegionServer内部



![img](https://pic3.zhimg.com/v2-6f292c56780bad2428f925c473ff9b82_r.jpg)



前面也提到了，HBase可以存储海量的数据，HBase是分布式的。所以我们可以断定：**HBase一张表的数据会分到多台机器上的**。那HBase是怎么切割一张表的数据的呢？用的就是**RowKey**来切分，其实就是表的**横向**切割。



![img](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/v2-33dd3cf742d701f91f9e3f2ccd74230b_r.jpg)



说白了就是一个**HRegion**上，存储HBase表的一部分数据。



![img](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/v2-623be466ec8ea2ad9c48b79f1d842023_r.jpg)



HRegion下面有**Store**，那Store是什么呢？我们前面也说过，一个HBase表首先要定义**列族**，然后列是在列族之下的，列可以随意添加。

**一个列族的数据是存储**在一起的，所以一个列族的数据是存储在一个Store里边的。

看到这里，其实我们可以认为HBase是基于列族存储的（毕竟物理存储，一个列族是存储到同一个Store里的）



![img](https://pic3.zhimg.com/v2-acbb43541256b6076cd9939035a718f6_r.jpg)



Store里边有啥？有`Mem Store、Store File、HFile`，我们再来看看里边都代表啥含义。



![img](https://pic3.zhimg.com/v2-bb0152a17990f0572c97b7076d4a1a62_r.jpg)



HBase在写数据的时候，会先写到`Mem Store`，当`MemStore`超过一定阈值，就会将内存中的数据刷写到硬盘上，形成**StoreFile**，而`StoreFile`底层是以`HFile`的格式保存，`HFile`是HBase中`KeyValue`数据的存储格式。

所以说：`Mem Store`我们可以理解为内存 buffer，`HFile`是HBase实际存储的数据格式，而`StoreFile`只是HBase里的一个名字。

#### HLog

写数据的时候是先写到内存的，**为了防止机器宕机，内存的数据没刷到磁盘中就挂了。我们在写`Mem store`的时候还会写一份`HLog`。**

这个`HLog`是顺序写到磁盘的，速度很快。

每个Region服务器只有一个HLog文件，而不是每个Region对象使用一个。优缺点p82

#### 总结

- HRegionServer是真正干活的机器（用于与hdfs交互），我们HBase表用RowKey来横向切分表
- HRegion里边会有多个Store，每个Store其实就是一个列族的数据（所以我们可以说HBase是基于列族存储的）
- Store里边有Men Store和StoreFile(HFile)，其实就是先走一层内存，然后再刷到磁盘的结构



### HMaster服务器

HMaster会处理 HRegion 的分配或转移。如果我们HRegion的数据量太大的话，HMaster会对拆分后的Region**重新分配RegionServer**。（如果发现失效的HRegion，也会将失效的HRegion分配到正常的HRegionServer中）

HMaster会处理元数据的变更和监控RegionServer的状态。





### HBase编程实践（大题）

- create创建表：create '表名', {NAME => '列族', VERSION => 版本号};
- put向表，行，列指定单元格添加数据：put '表名', '行', '列名f1:c1', '数据', '时间戳'
- scan浏览表的相关信息：scan '表名', {COLUMNS => '列名'} 
- alter修改列族：
  - 添加：alter '表名' ，NAME => '列族'
  - 删除：alter '表名' ，NAME => '列族', METHOD => 'delete'
  - 设定列族最大为128MB：alter '表名' ，METHOD => 'table_att', MAX_FILESIZE => '134217728'
- describle显示表相关信息：descirble '表名'





# NoSql数据库

### nosql概述

>nosql数据库没有固定的表结构，通常也不存在连接操作，也没有严格遵守ACID约束。
>
>相比与关系型数据库，具有灵活的可拓展性，可以支持海量数据存储。

#### **nosql三个特点：**

- 灵活的可拓展性
- 灵活的数据模型
- 与云计算紧密融合

#### NoSql的四大类型（简答？）

1. 键值数据库
2. 列族数据库
3. 文档数据库
4. 图数据库



### NoSql与RDS的对比

![image-20220105211619305](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/image-20220105211619305.png)

<img src="https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/image-20220105180256898.png" alt="image-20220105180256898"  />

## NOSql的三大基石（简答，填空）

> CAP原则，Base理论，最终一致性

### CAP原则

　　CAP原则又称CAP定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），**三者不可得兼。**

分布式系统的CAP理论：理论首先把分布式系统中的三个特性进行了如下归纳：

- 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）
- 可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性）
- 分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。



**CAP三个特性只能满足其中两个，那么取舍的策略就共有三种：**

**CA without P：**如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。**传统的关系型数据库RDBMS：Oracle、**MySQL就是CA。

**CP without A：**如果不要求A（可用），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是分布式数据库，**如Redis、HBase等**。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。

 **AP wihtout C：**要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。

**取舍：**AP

​	对于web2.0网站而言，可用性与分区容忍性优先级高于数据一致性，**网站一般会尽量朝AP的方向设计。**



### BASE理论

BASE是Basically Available（**基本可用**）、Soft state（**软状态**）和Eventually consistent（**最终一致性**）三个短语的简写，**BASE是对CAP中一致性和可用性权衡的结果**，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其**核心思想**是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。接下来我们着重对BASE中的三要素进行详细讲解。

#### 基本可用

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子。

- 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。
- 功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。

#### **弱状态**

也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据听不的过程存在延时。

#### 最终一致性

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性



### 最终一致性再探讨

**因果一致性：**

​    因果一致性是指，如果进程A在更新完某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新情况。与此同时，与进程A无因果关系的进程C的数据访问则没有这样的限制。

**读己之所写：**

​    读己之所写是指，进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。也就是说，对于单个数据获取者而言，其读取到的数据一定不会比自己上次写入的值旧。因此，读己之所写也可以看作是一种特殊的因果一致性。

**会话一致性：**

​    会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。

**单调读一致性：**

​    单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。

**单调写一致性：**

​     单调写一致性是指，一个系统需要能够保证来自同一个进程的写操作被顺序地执行。



**试解释数据库的ACID四性的含义？**

- Atomicity（原子性）：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。
- Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。
- Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。
- Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。





# MapReduce

> 分布式并行编程模型



MapReduce擅长处理大数据，它为什么具有这种能力呢？这可由MapReduce的设计思想发觉。MapReduce的思想就是“**分而治之**”。

　　（1）**Mapper负责“分”**，即把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：

一是数据或计算的规模相对原任务要大大**缩小**；二是**就近计算原则**，即任务会分配到存放着所需数据的节点上进行计算；三是这些小任务**可以并行计算，彼此间几乎没有依赖**关系。

　　（2）**Reducer**负责对map阶段的**结果进行汇总**。至于需要多少个Reducer，用户可以根据具体问题，通过在mapred-site.xml配置文件里设置参数mapred.reduce.tasks的值，缺省值为1。

> 一个比较形象的语言解释MapReduce：　　
> 我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“**Map**”。我们人越多，数书就更快。
> 现在我们到一起，把所有人的统计数加在一起。这就是“**Reduce**”。

**设计理念**：计算向数据靠拢

### 关系代数运算（大题）

- 选择
- 投影
- 并，交，差
- **自然连接（大题？）**

![image-20220105202331572](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/image-20220105202331572.png)

#### 分组与聚合运算？

词频计算就是典型的分组聚合运算。

**分组是使用特定的条件将元数据进行划分为多个组。聚合是对每个分组中的数据执行某些操作，最后将计算结果进行整合。**

分组与聚合的过程大概分三步：

1. 拆分：将数据集按照一些标准拆分为若干组。
2. 应用：将某个函数或者方法应用到每个分组。
3. 合并：将产生的新值整合到结果对象中。

#### 矩阵向量乘法？

> Map函数与Reduce函数

![image-20220105211201437](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/image-20220105211201437.png)



# 零散知识点

### 云数据库（填空）

### 云计算

>云计算是云数据库兴起的基础

云计算包括：IaaS,PaaS,SaaS。

现阶段所说的**云计算服务**已经不单单是一种分布式计算，而是分布式计算、效用计算、负载均衡、并行计算、网络存储、热备份冗杂和虚拟化等计算机技术混合演进并跃升的结果。



## 针对Hadoop的改进与提升（简答）

![image-20220105211413190](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/image-20220105211413190.png)



## Spark运行基本流程（简答）



**1）**构建 Spark Application 的运行环境（由任务控制节点Driver创建一个SparkContext），SparkContext 向资源管理器 Cluster Manager 注册，并申请运行 Executor 资源。

**2）**资源管理器 Cluster Manager 为 Executor 分配资源并启动 Executor 进程，Executor 运行情况将随着“心跳”发送到 资源管理器 Cluster Manager 上。

![Spark运行基本流程图](http://c.biancheng.net/uploads/allimg/190515/5-1Z5151253501C.gif)

**3）**SparkContext 构建 DGA 图，将 DAG 图分解成多个 Stage，并把每个 Stage 的 TaskSet（任务集）发送给 Task Scheduler (任务调度器）。Executor 向 SparkContext 申请 Task, 任务调度器Task Scheduler 将 Task 发放给 Executor,同时，SparkContext 将应用程序代码发放给 Executor。

**4）**Task 在 Executor 上运行，把执行结果反馈给任务调度器Task Scheduler，然后再反馈给 DAG Scheduler。运行完毕后，写入数据，SparkContext 向 ClusterManager 注销并释放所有资源。

![image-20220105205342926](https://gitee.com/diaborosu/raisei-pic-md/raw/master/image/image-20220105205342926.png)



