
# 原文

## Abstract
There are many limitations applying object detection algorithm on various environments. Especially detecting small objects is still challenging because they have low- resolution and limited information. We propose an object detection method using context for improving accuracy of detecting small objects. The proposed method uses addi- tional features from different layers as context by concate- nating multi-scale features. We also propose object detec- tion with attention mechanism which can focus on the ob- ject in image, and it can include contextual information from target layer. Experimental results shows that proposed method also has higher accuracy than conventional SSD on detecting small objects. Also, for 300×300 input, we achieved 78.1% Mean Average Precision (mAP) on the PAS- CAL VOC2007 test set.


## 1. Introduction

Object detection is one of key topics in computer vision which th goals are finding bounding box of objects and their classification given an image. In recent years, there has been huge improvements in accuracy and speed with the lead of deep learning technology: Faster R-CNN [13] achieved 73.2% mAP, YOLOv2 [12] achieved 76.8% mAP, SSD [10] achieved 77.5% mAP. However, there still remains important challenges in detecting small objects. For example, SSD can only achieved 20.7% mAP on small objects targets. Figure 1 shows the failure cases when SSD cannot detect the small objects. There are still a lot of room to improve in small object detection.

In this paper, we propose to use context information ob- ject for tackling the challenging problem of detecting small objects. First, to provide enough information on small ob- jects, we extract context information from surrounded pix- els of small objects by utilizing more abstract features from higher layers for the context of an object. By concatenat- ing the features of an small object and the features of the context, we augment the information for small objects so that the detector can detect the objects better. Second, to focus on the small object, we use an attention mechanism in the early layer. This is also help to reduce unnecessary shallow features information from background. We select Single Shot Multibox Detector (SSD) [10] for our baseline in our experiments. However, the idea can be generalize to other networks. In order to evaluate the performance of the proposed model, we train our model to PASCAL VOC2007and VOC2012 [1], and comparison with baseline and state- of-the-art methods on VOC2007 will be given.

## 2. Related Works

Object detection with deep learning The advancement of deep learning technology has been improving the accu- racy of object detection greatly. The first try for object de- tection with deep learning was R-CNN [4]. R-CNN uses Convolutional Neural Network(CNN) on region proposals generated by using selective search [16]. It is, however, too slow for real-time applications since each proposed region goes through CNNs sequentially. Fast R-CNN[3] is faster than R-CNN because it performs feature extraction stage only once for all the region proposals. But those two works still use separate stage for region proposals, which becomes the main tackling point by Faster R-CNN [13] combines the region proposal phase and classification phase into one model such that it allows so-called end-to-end learning. Object detection technologies has even been accelerated by YOLO [11] and SSD [10] showing high performance enough for real-time object detection. However, they are still not showing good performance for small objects. Small object detection Recently, several ideas has been proposed for detecting small object [10, 2, 7, 8]. Liu et al [10] augmented small object data by reducing the size of large objects for overcoming the not-enough-data prob- lem. Besides the approach for data augmentation, there has been some efforts for augmenting the required information without augmenting dataset perse. DSSD [2] applies de- convolution technique on all the feature maps of SSD to obtain scaled-up feature maps. However, it has the limita- tion of increased model complexity and slow down an speed due to applying deconvolution module to all feature maps. R-SSD [7] combines features of different scales through pooling and deconvolution and obtained improved accuracy and speed compared to DSSD. Li et al [8] uses Generative Adversarial Network(GAN) [5] to generate high-resolution features using low-resolution features as input to GAN. Visual attention network Attention mechanism in deep learning can be broadly understood as focusing on part of input for solving specific task rather than seeing the entire input. Thus, attention mechanism is quite similar to what humans do when we see or hear something, Xu et al [18] uses visual attention to generate image captions. In order to generate caption corresponding to images, they used Long Short-Term Memory(LSTM) and the LSTM takes a relevant part of a given image. Sharm et al [14] applied attention mechanism to recognize actions in video. Wang et al [17] improved classification performance on ImageNet dataset by stacking residual attention modules.

## 3. Method

This section will discuss the baseline SSD, then followed by the components we propose to improve small object de- tection capability. First, SSD with feature fusion to get the context information, named F-SSD. Second, SSD with at- tention module to give the network capability to focus on important parts, named A-SSD. Third, we combine both feature fusion and attention module, named FA-SSD. 3.1. Single Shot Multibox Detector (SSD) In this section, we review Single Shot Multibox Detector (SSD) [10], which we are going to improve the capability on detecting small object. Like YOLO [11], it is a one-stage detector which goal is to improve the speed, while also im- proving the detection in different scales by processing dif- ferent level of feature maps, as seen in Fig. 3a. The idea is utilizing the higher resolution of early feature maps to detect smaller objects while the deeper feature which has lower resolution for the larger object detection. It is based on VGG16 [15] backbone with additional lay- ers to create different resolution of feature maps, as seen in Fig. 3a. From each of the features, with one additional con- volution layer to match the output channels, the network predicts the output that consists both the bounding box re- gression and object classification. However, the performance on small objects is still low, 20.7% on VOC 2007, hence there are still many room for improvement. We believe there are two main reasons. First, the lack of context information to detect small object. On top of that, the features for small object detection are taken from shallow features which lack of semantic information. Our goal is to improve the SSD by adding feature fusion to solve the two problems. In addition, to improve more, we add attention module to make the network focuses only on the important part.


### 3.2. F-SSD: SSD with context by feature fusion

In order to provide context for a given feature map (tar- get feature) where we want to detect objects, we fuse it with feature maps (context features) from higher layers that the layer of the target features. For example in SSD, given our target feature from conv4_3, our context features are com- ing from two layers, they are conv7 and conv8_2, as seen in Fig. 3.2. Although our feature fusion can be generalized to any target feature and any of its higher features. However, those feature maps have different spatial size, therefore we propose fusion method as described in Fig. 4. Before fus- ing by concatenating the features, we perform deconvolu- tion on the context features so they have same spatial size

with the target feature. We set the context features channels to the half of the target features so the amount of context information is not overwhelming the target features itself. Just for the F-SSD, we also add one extra convolution layer to the target features that does not change the spatial size and number of channels. Furthermore, before concatenating features, a normalization step is very important because each feature values in different layers have different scale. Therefore, we perform batch normalization and ReLU af- ter each layer. Finally, we concatenate target features and context features by stacking the features.

### 3.3. A-SSD: SSD with attention module

Visual attention mechanism allows for focusing on part of an image rather than seeing the entire area. Inspired by the success of residual attention module proposed by Wang et al [17], we adopt the residual attention module for ob- ject detection. For our A-SSD (Fig. 3.3), we put two-stages residual attention modules after conv4_3 and conv7. Al- though it can be generalized to any of layers. Each of the residual attention stage can be described on Fig. 5. It con- sists of a trunk branch and a mask branch. The trunk branch has two residual blocks, of each has 3 convolution layers as in Fig. 5d. The mask branch outputs the attention maps by performing down-sampling and up-sampling with residual connection (Fig. 5b for the first stage and Fig.5c for the sec- ond stage), then finalized with sigmoid activation. Residual connections makes the features in down-sampling phase are maintained. The attention maps from the mask branch are then multiplied with the output of trunk branch, producing attended features. Finally, the attended features are followed by another residual block, L2 normalization, and ReLU.
### 3.4. FA-SSD: Combining feature fusion and attention in SSD

We propose method for concatenating two features pro- posed in section 3.2 and 3.3, it can consider context infor- mation from the target layer and different layer. Compare with F-SSD, instead of performing one convolution layer on the target feature, we put one stage attention module, as seen in Fig. 3d. The feature fusion method (Fig.4) is same.

## 4. Experiments

### 4.1. Experimental setup

We applied the proposed method to SSD [10] with same augmentation 1. We use SSD with VGG16 backbone and 300 × 300 input, unless specified otherwise. For FA-SSD, we applied feature fusion method to conv4_3 and conv7 of SSD. With conv4_3 as a target, conv7 and conv8_2 are used as context layers, and with conv7 as a target, conv8_2 and conv9_2 are used as context layers. We apply attention module on lower 2 layers for detecting small object. The output of attention module has equal size with target features. We trained our models with PASCAL VOC2007 and VOC2012 trainval datasets with learning rate 10−3 for first 80k iterations, then decreased to 10−4 and 10−5 for 100k and 120k iterations, batch size was 16. All of test results are tested with VOC2007 test dataset and we follows COCO [9] for objects size classification, which small objects area is less than 32_32 and large objects area is greater than 96_96. We train and test using PyTorch and Titan Xp machine.

### 4.2. Ablation studies

To test on the importance of each feature fusion and at- tention components compare with SSD baseline, we com- pare the performance between SSD, F-SSD, A-SSD, and FA-SSD. Table 1 shows that all F-SSD, A-SSD are bet- ter than the SSD which means each components improves the baseline. Although combining fusion and attention as FA-SSD does not show better overall performance compare with F-SSD, FA-SSD shows the best performance and sig- nificant improvement on the small objects detection.

### 4.3. Inference time

One interesting thing from results on Table 1 is that the speed does not always be slower with more compo- nents. This motivates us to see the inference time in more detail. Inference time in detection is divided by two, the network inference and the post processing which includes Non-Maximum Suppression (NMS). Based on Table 2, al- though SSD has the fastest forwarding time, it is the slowest during post processing, hence in total it is still slower than F-SSD and A-SSD.

### 4.4. Qualitative results

Figure 7 shows the comparison between SSD and FA- SSD qualitatively where SSD fails on detecting small ob- jects when FA-SSD succeeds.

### 4.5. Attention visualization

In order to have more understanding on the attention module, we visualize the attention mask from FA-SSD. The attention mask is taken after sigmoid function on Fig. 5a. There are many channels on the attention mask, 512 chan- nels from conv4_3 and 1024 channels from conv7. Each of the channels focuses on different things, both the object and the context. We visualize some samples of the attention masks on Fig. 8.

### 4.6. Generalization on ResNet backbones

In order to know the generalization with different back- bones of SSD, we experiment with ResNet [6] architectures, specifically ResNet18, ResNet34, and ResNet50. To make the features size same with the original SSD with VGG16 backbone, we take the features from layer 2 results (Fig. 6a). Then F-SSD (Fig. 6b), A-SSD (Fig. 6c), and FA-SSD (Fig. 6d) just follow the VGG16 backbone version. As seen in Ta- ble 3, everything follow the trend of the VGG16 backbone version in Table 1, except the ResNet34 backbone version does not have the best performance on the small object.

### 4.7. Results on VOC2007 test

For comparison with other works we compare in Table 4. All of the methods compared are trained with VOC2007 trainval and VOC2012 trainval datasets. Although we have lower performance compare to DSSD [2], our approach runs on 30 FPS while DSSD runs on 12 FPS.

## 5. Conclusion

In this paper, to improve accuracy for detecting small object, we presented the method for adding context-aware

ferent layer by fusing multi-scale features and shown on target layer by applying attention mechanism. Our experi- ments show improvement in object detection accuracy compared to conventional SSD, especially achieve significantly enhancement for small object.


# ChatGPT翻译

## 摘要
在各种环境中应用目标检测算法存在许多限制。尤其是检测小物体仍然具有挑战性，因为它们具有低分辨率和有限信息。我们提出了一种使用上下文改进小物体检测准确性的目标检测方法。所提出的方法通过连接多尺度特征来使用来自不同层的附加上下文特征。我们还提出了具有注意机制的目标检测方法，它可以聚焦于图像中的目标，并可以包含来自目标层的上下文信息。实验结果表明，所提出的方法在检测小物体方面的准确性也高于传统的SSD。此外，对于300×300的输入，在PASCAL VOC2007测试集上实现了78.1％的平均精度（mAP）。

## 1. 引言
目标检测是计算机视觉中的关键主题之一，其目标是在给定图像中找到物体的边界框并进行分类。近年来，随着深度学习技术的引领，目标检测的准确性和速度取得了巨大的改进：Faster R-CNN [13] 实现了73.2％的mAP，YOLOv2 [12] 实现了76.8％的mAP，SSD [10] 实现了77.5％的mAP。然而，在检测小物体方面仍然存在重要的挑战。例如，SSD只能在小物体目标上实现20.7％的mAP。图1显示了当SSD无法检测到小物体时的失败案例。在小物体检测方面仍有很大的改进空间。

在本文中，我们提出使用上下文信息来解决检测小物体这一具有挑战性问题。首先，为了提供足够的小物体信息，我们从较高层的抽象特征中提取小物体周围像素的上下文信息。通过将小物体的特征和上下文的特征进行连接，我们增加了小物体的信息，使得检测器能够更好地检测物体。其次，为了聚焦于小物体，我们在早期层次中使用了注意机制。这也有助于减少来自背景的不必要的浅层特征信息。我们选择Single Shot Multibox Detector (SSD) [10]作为我们实验中的基线模型。然而，这个想法可以推广到其他网络。为了评估所提出模型的性能，我们将模型训练到PASCAL VOC2007和VOC2012 [1]上，并与基线模型和最先进的方法在VOC2007上进行比较。

## 2. 相关工作
深度学习的目标检测随着技术的进步已经大大提高了目标检测的准确性。深度学习进行目标检测的第一次尝试是R-CNN [4]。R-CNN使用卷积神经网络（CNN）对使用选择性搜索[16]生成的区域建议进行处理。然而，由于每个建议区域都要经过CNN顺序处理，因此它对于实时应用来说速度太慢。Fast R-CNN[3]比R-CNN更快，因为它仅对所有区域建议执行一次特征提取阶段。但是这两个方法仍然使用了区域建议的单独阶段，这成为Faster R-CNN [13]解决的主要问题，它将区域建议阶段和分类阶段组合成一个模型，从而实现所谓的端到端学习。YOLO [11]和SSD [10]通过提供高性能的实时目标检测进一步加快了目标检测的速度。然而，它们在小物体上的性能仍然不佳。最近，已经提出了一些用于检测小物体的方法[10, 2, 7, 8]。Liu等人[10]通过减小大物体的尺寸来增加小物体数据，以克服数据不足的问题。除了数据增强的方法外，还有一些方法在不增加数据集的情况下增加所需的信息。DSSD [2]对SSD的所有特征图应用反卷积技术以获得放大的特征图。然而，这样做增加了模型的复杂性，并且由于将反卷积模块应用于所有特征图，导致速度变慢。R-SSD [7]通过池化和反卷积结合不同尺度的特征，获得了比DSSD更高的准确性和速度。Li等人[8]使用生成对抗网络（GAN）[5]，将低分辨率特征作为GAN的输入，生成高分辨率特征。注意网络在深度学习中的注意机制可以广泛地理解为专注于解决特定任务的输入的一部分，而不是看到整个输入。因此，注意机制与人类在看到或听到某些事物时所做的事情非常相似。Xu等人[18]使用视觉注意力生成图像标题。为了生成与图像对应的标题，他们使用了长短期记忆（LSTM），并且LSTM获取给定图像的相关部分。Sharm等人[14]应用注意机制来识别视频中的动作。Wang等人[17]通过堆叠残差注意模块来改善ImageNet数据集上的分类性能。

## 3. 方法
本节将讨论基线SSD，然后介绍我们提出的用于改进小物体检测能力的组件。首先，使用特征融合的SSD，命名为F-SSD。其次，使用注意模块的SSD，使网络能够专注于重要部分，命名为A-SSD。第三，我们将特征融合和注意模块结合起来，命名为FA-SSD。

### 3.1. 单次多框检测器（SSD）
在本节中，我们回顾一下单次多框检测器（SSD）[10]，我们将改进其对小物体检测的能力。与YOLO [11]类似，SSD是一个单阶段检测器，其目标是提高速度，同时通过处理不同级别的特征图来改进不同尺度的检测，如图3a所示。其基本思想是利用早期特征图的更高分辨率来检测较小的物体，而深层特征则用于较大物体的检测，因为深层特征的分辨率较低。它基于VGG16 [15]骨干网络，并通过额外的层来创建不同分辨率的特征图，如图3a所示。从每个特征图中，通过一个额外的卷积层来匹配输出通道，网络预测包含边界框回归和物体分类的输出。然而，对于小物体，其性能仍然较低，在VOC 2007上为20.7％，因此仍然有很大的改进空间。我们认为有两个主要原因。首先，缺乏检测小物体的上下文信息。此外，用于小物体检测的特征来自浅层特征，缺乏语义信息。我们的目标是通过添加特征融合来改进SSD以解决这两个问题。此外，为了进一步改进，我们添加了注意模块，使网络仅关注重要部分。

### 3.2. F-SSD：通过特征融合添加上下文的SSD
为了为我们想要检测物体的特征图（目标特征）提供上下文信息，我们将其与来自比目标特征层级更高层级的特征图（上下文特征）进行融合。例如，在SSD中，给定我们从conv4_3得到的目标特征，我们的上下文特征来自两个层级，即conv7和conv8_2，如图3.2所示。尽管我们的特征融合可以推广到任何目标特征和其上层特征。然而，这些特征图具有不同的空间大小，因此我们在图4中描述了融合方法。在通过连接特征之前，我们对上下文特征进行反卷积，使其具有与目标特征相同的空间大小。我们将上下文特征的通道数设置为目标特征的一半，以确保上下文信息的数量不会超过目标特征本身。对于F-SSD，我们还在目标特征上添加了一个额外的卷积层，该层不改变空间大小和通道数。此外，在连接特征之前，归一化步骤非常重要，因为不同层中的每个特征值具有不同的尺度。因此，我们在每个层之后执行批量归一化和ReLU。最后，我们通过堆叠特征来连接目标特征和上下文特征。

### 3.3. A-SSD：带有注意模块的SSD
视觉注意机制允许专注于图像的某一部分，而不是看到整个区域。受Wang等人提出的残差注意模块的成功启发，我们采用了残差注意模块进行目标检测。对于我们的A-SSD（图3.3），我们在conv4_3和conv7之后放置了两个阶段的残差注意模块。尽管它可以推广到任何层级，但每个残差注意阶段的描述如图5所示。它由一个主干分支和一个掩码分支组成。主干分支有两个残差块，每个残差块有3个卷积层，如图5d所示。掩码分支通过执行下采样和上采样以及残差连接（第一个阶段为图5b，第二个阶段为图5c）输出注意力图，最后使用sigmoid激活。残差连接使得下采样阶段的特征保持不变。掩码分支的注意力图然后与主干分支的输出相乘，产生注意特征。最后，注意特征经过另一个残差块、L2归一化和ReLU处理。

### 3.4. FA-SSD：在SSD中结合特征融合和注意力
我们提出了一种在第3.2节和第3.3节中提出的两个特征进行连接的方法，该方法可以考虑目标层和不同层的上下文信息。与F-SSD相比，我们在目标特征上放置了一个阶段的注意模块，如图3d所示。特征融合方法（图4）相同。

## 4. 实验

### 4.1. 实验设置
我们将所提出的方法应用于SSD [10]，采用相同的数据增强方法1。除非另有说明，我们使用带有VGG16骨干网络和300×300输入的SSD。对于FA-SSD，我们将特征融合方法应用于SSD的conv4_3和conv7。以conv4_3为目标，使用conv7和conv8_2作为上下文层，以conv7为目标，使用conv8_2和conv9_2作为上下文层。我们在较低的2个层级上应用注意模块以检测小物体。注意模块的输出与目标特征具有相同的大小。我们使用PASCAL VOC2007和VOC2012 trainval数据集对模型进行训练，并与基线模型和最先进的方法在VOC2007上进行比较。我们使用PyTorch和Titan Xp机器进行训练和测试。

### 4.2. 消融研究
为了测试与SSD基线相比，特征融合和注意力组件的重要性，我们比较了SSD、F-SSD、A-SSD和FA-SSD的性能。表1显示了F-SSD和A-SSD都优于SSD，这意味着每个组件都改进了基线模型。尽管将特征融合和注意模块结合在一起的FA-SSD在整体性能上并没有表现得更好，但FA-SSD在小物体检测方面表现出最佳性能并有显著改进。

### 4.3. 推理时间
表1中的有趣之处在于，随着组件的增加，速度并不总是变慢。这激励我们更详细地观察推理时间。检测中的推理时间分为两部分，网络推理和后处理，包括非最大抑制（NMS）。根据表2，尽管SSD的前向传播时间最快，但在后处理过程中最慢，因此总体上比F-SSD和A-SSD还要慢。

### 4.4. 定性结果
图7 qualitatively显示了SSD和FA-SSD在检测小物体方面的比较，SSD无法检测到小物体，而FA-SSD成功检测到了它们。

### 4.5. 注意力可视化
为了更好地理解注意模块，我们可视化了FA-SSD中的注意力掩码。注意力掩码在图5a上经过sigmoid函数处理。注意力掩码有许多通道，来自conv4_3的512个通道和conv7的1024个通道。每个通道专注于不同的物体和上下文。我们在图8上可视化了一些注意力掩码的示例。

### 4.6. 在ResNet骨干网络上的泛化
为了了解在不同SSD骨干网络上的泛化情况，我们使用ResNet [6]架构进行实验，具体是ResNet18、ResNet34和ResNet50。为了使特征的大小与具有VGG16骨干网络的原始SSD相同，我们从layer 2的结果（图6a）中获取特征。然后F-SSD（图6b）、A-SSD（图6c）和FA-SSD（图6d）与VGG16骨干网络版本保持一致。如表3所示，除了ResNet34骨干网络版本在小物体上的性能不如其他版本外，其他都与VGG16骨干网络版本的趋势相同。

### 4.7. VOC2007测试结果
为了与其他工作进行比较，我们在表4中进行了比较。所有比较的方法都使用了VOC2007 trainval和VOC2012 trainval数据集进行训练。尽管与DSSD [2]相比，我们的方法的性能较低，但我们的方法在30 FPS的速度下运行，而DSSD在12 FPS的速度下运行。

## 5. 结论
本文提出了一种通过特征融合和注意力来增加上下文信息的方法，以提高检测小物体的准确性。实验结果表明，与传统的SSD相比，我们的方法在目标检测准确性方面有所改进，特别是在小物体检测方面取得了显著的改进。