---
tags:
  - 理论概念
  - 机器学习
---

参考教材：
模式识别，第4版，张学工，汪小我，清华大学出版社
[[../../机器学习/机器学习|机器学习]]方法，李航，清华大学出版社

## 考点

1. 模式，模式识别，监督模式识别和非监督模式识别。
2. 机器学习方法三要素，模型评估与模型选择，正则化与交叉验证，泛化能力。
3. 最小错误贝叶斯决策，最小风险贝叶斯决策，两类错误，正确率，召回率，准确率，ROC曲线，正态分布概率模型下最小错误贝叶斯决策。
4. 最大似然估计的基本原理，正态分布下的最大似然估计，贝叶斯估计，正态分布时的贝叶斯估计，非参数估计的直方图估计方法原理。
5. 隐马尔可夫模型基本原理，EM算法基本原理，朴素贝叶斯分类器基本原理。
6. 线性判别函数的基本概念，线性判别分析LDA，感知器基本原理与梯度下降方法，最优分类面与线性支持向量机以及线性不可分支持向量机，多类线性分类器的基本思路。
7. 多层感知器神经网络，支持向量机与核函数。
8. 最近邻法，k近邻法，ID3方法，随机森林，Adaboost算法。
9. 主成分分析PCA，K-L变换，MDS方法，LLE方法，t-SNE方法，奇异值分解，潜在语义分析。
10. c均值聚类算法，分级聚类算法。
11. DNN，CNN，RNN，GAN。

# 模式识别

**模式是指数据中的可重复的结构或趋势。模式可以是在数据中出现的规律、特征或者是数据点之间的关系。模式识别是指识别和理解数据中的这些模式的过程，这些模式可能存在于不同类型的数据中，包括文本、图像、音频等。**

**监督模式识别**是一种机器学习任务，其目标是从有标签的数据中学习并建立模型，然后使用这个模型对未知数据进行预测或分类。在监督模式识别中，算法通过学习输入数据与其对应的输出标签之间的关系来建立模型。例如，给定一组包含猫和狗图像的数据集，并且每张图像都有相应的标签（猫或狗），监督学习算法将学习如何从图像中提取特征，并预测未知图像中的动物是猫还是狗。

**非监督模式识别**是一种机器学习任务，其目标是在没有标签的数据中发现隐藏的结构或模式。在非监督模式识别中，算法需要自主地从数据中学习，而不是依赖于预先定义的输出标签。这意味着算法必须自行发现数据中的结构或者将数据点进行聚类。例如，聚类算法可以用于将相似的数据点分组在一起，而无需先验知识告诉算法哪些数据点属于同一组。

总的来说，监督模式识别依赖于有标签的训练数据，而非监督模式识别则不依赖于标签，而是依赖于数据本身的结构和特点。

# 机器学习三要素

机器学习方法通常由三个要素组成：

1. **模型**：模型是机器学习算法的数学表示，它用于从数据中学习模式和关系。模型可以是简单的线性方程或复杂的神经网络，其选择取决于任务的性质和数据的特征。
    
2. **优化算法**：优化算法是用来调整模型参数以使其与训练数据拟合得更好的算法。这些算法通过最小化损失函数来优化模型，使其能够更好地预测未知数据的标签或属性。
    
3. **损失函数**：损失函数衡量了模型预测结果与实际结果之间的差距。优化算法的目标就是最小化损失函数。常见的损失函数包括均方误差（Mean Squared Error）、交叉熵（Cross Entropy）等。
    

# 模型训练

模型评估与模型选择是机器学习中至关重要的一部分：

1. **模型评估**：模型评估是指评估模型在未知数据上的性能。通常，将数据集分为训练集和测试集，用训练集训练模型，然后在测试集上评估模型的性能。评估指标可以是准确率、精确度、召回率、F1 分数等，具体选择取决于任务类型。
    
2. **模型选择**：模型选择是指从多个候选模型中选择最佳模型的过程。在模型选择中，需要比较不同模型在验证集（或交叉验证）上的性能，并选择性能最好的模型。常见的模型选择方法包括交叉验证、网格搜索等。
    

正则化与交叉验证是用来提高模型泛化能力的技术：

1. **正则化**：正则化是一种减少模型过拟合的技术。它通过在**损失函数中添加一个正则化项来惩罚模型的复杂度**，使得模型不会过度拟合训练数据，从而提高其在未知数据上的性能。
    
2. **交叉验证**：交叉验证是一种评估模型泛化能力的统计学方法。它通过将数据集分成多个子集，然后多次训练模型并在不同的子集上进行测试，从而得到更准确的模型性能评估。常见的交叉验证方法包括 K 折交叉验证、留一交叉验证等。
    
**泛化能力**是指模型在未知数据上的表现能力。一个具有良好泛化能力的模型能够在未见过的数据上做出准确的预测，而不仅仅是在训练数据上表现良好。提高模型泛化能力是机器学习中的重要目标之一，可以通过正则化、交叉验证等方法来实现。

### 基础概率概念

![[../../attachments/d6eb4c92b037af99ffa39d00e88f8675_MD5.png]]

贝叶斯决策会选择使**后验概率最大化**的类别作为样本的分类结果。
**错误**指的是对样本进行错误分类所带来的损失。**风险**则是在给定样本和可能的决策下，所造成的平均损失或者期望损失。
![[../../attachments/86585c20b3b03b24de2abc207fd68fee_MD5.png]]
1. **真正例（True Positive, TP）**：样本的真实类别是正类（Positive），并且分类器将其正确地预测为正类。
2. **真负例（True Negative, TN）**：样本的真实类别是负类（Negative），并且分类器将其正确地预测为负类。
3. **假正例（False Positive, FP）**：样本的真实类别是负类，但分类器错误地将其预测为正类。
4. **假负例（False Negative, FN）**：样本的真实类别是正类，但分类器错误地将其预测为负类。

# 评估指标

1. **最小错误贝叶斯决策**：最小错误贝叶斯决策是一种决策规则，它通过最小化错误率来选择分类结果。错误率可以是分类错误的样本数除以总样本数。最小错误贝叶斯决策的目标是选择具有最小错误率的类别作为样本的分类结果。

2. **最小风险贝叶斯决策**：最小风险贝叶斯决策是一种决策规则，它通过最小化分类风险来选择分类结果。分类风险考虑了不同类型错误的成本，并将其纳入决策过程中。最小风险贝叶斯决策的目标是选择使期望分类风险最小化的类别作为样本的分类结果。

3. **两类错误**：在二分类问题中，有两种类型的错误：假阳性和假阴性。假阳性是指将负样本错误地分类为正样本，而假阴性是指将正样本错误地分类为负样本。

4. **正确率（Accuracy）**：正确率是指被正确分类的样本数占总样本数的比例。它是衡量分类器性能的常用指标，计算公式为：$\[ Accuracy = \frac{TP + TN}{TP + TN + FP + FN} \]$其中，TP表示真正例（True Positive），TN表示真负例（True Negative），FP表示假正例（False Positive），FN表示假负例（False Negative）。

5. **召回率（Recall）**：召回率是指分类器正确识别正样本的能力。计算公式为：$\[ Recall = \frac{TP}{TP + FN} \]$

6. **准确率（Precision）**：准确率是指分类器正确识别出的正样本在所有被分类为正样本中的比例。计算公式为：$\[ Precision = \frac{TP}{TP + FP} \]$

7. **ROC曲线**：ROC曲线是一种用于评估二分类器性能的图形工具。它以假阳性率（False Positive Rate）为横轴，真阳性率（True Positive Rate，召回率的另一种表示）为纵轴绘制，曲线下的面积称为AUC（Area Under Curve）。ROC曲线越接近左上角，分类器的性能越好。

8. **正态分布概率模型下最小错误贝叶斯决策**：在正态分布概率模型下，假设样本的特征在每个类别下都服从正态分布。最小错误贝叶斯决策将通过计算后验概率，并选择具有最大后验概率的类别作为样本的分类结果。这个过程还涉及到计算每个类别下的先验概率以及每个特征在每个类别下的条件概率。

# 参数估计估计

### 最大似然估计的基本原理

最大似然估计（Maximum Likelihood Estimation, MLE）是一种参数估计方法，基于样本数据，寻找使得**样本出现的概率最大的参数值**。其基本原理是假设观测到了一些数据，然后尝试找到使这些数据出现的可能性最大的参数值。这可以通过对参数进行调整，使得观测数据的**概率密度函数最大化**来实现。

### 正态分布下的最大似然估计

在正态分布（也称为高斯分布）的情况下，最大似然估计是试图找到使观测数据点服从正态分布概率密度函数的参数。具体而言，对于正态分布的情况，我们试图找到均值（μ）和方差（σ²），使得观测数据的出现概率最大化。

### 贝叶斯估计

贝叶斯估计是一种基于贝叶斯定理的参数估计方法。在贝叶斯估计中，我们不仅利用观测数据，还引入了先验概率信息。它与最大似然估计的不同之处在于，它不仅仅寻找单个参数的点估计，而是**寻找参数的整个后验分布**。通过结合先验分布和观测数据，贝叶斯估计提供了对参数不确定性的更全面的理解。

### 正态分布时的贝叶斯估计

在正态分布情况下的贝叶斯估计，通常我们会假设参数（如均值和方差）的先验分布也是正态分布。然后利用贝叶斯定理，结合观测数据和先验分布，计算出参数的后验分布，从而得到对参数的估计。

### 非参数估计的直方图估计方法原理

直方图估计是一种非参数密度估计方法，用于估计数据的概率密度函数，而无需对数据分布进行假设。其原理是将数据集划分成若干个间隔（bin），然后计算每个间隔中数据点的频率，最终将频率转换为概率密度估计。直方图估计的精度受到间隔宽度的影响，不同的间隔宽度可能会导致不同的概率密度估计结果。

# 经典模型

### 隐马尔可夫模型（Hidden Markov Model, HMM）基本原理

隐马尔可夫模型是一种用于建模时序数据的概率模型。它包含两种类型的随机变量：观测变量和隐藏状态变量。在隐马尔可夫模型中，隐藏状态序列决定了观测序列的生成过程。具体而言，隐马尔可夫模型由三组参数组成：

1. **状态转移概率矩阵（Transition Probability Matrix）**：描述隐藏状态之间的转移概率。
2. **观测概率矩阵（Emission Probability Matrix）**：描述隐藏状态生成观测值的概率。
3. **初始状态概率分布（Initial State Probability Distribution）**：描述模型开始时各隐藏状态的概率分布。

隐马尔可夫模型的基本问题包括状态序列的推断、参数学习以及观测序列的概率计算。

	### EM算法（Expectation-  Algorithm）基本原理

EM算法是一种迭代优化算法，用于求解含有隐变量的概率模型参数估计问题。其基本思想是通过交替进行两个步骤来优化模型参数：E步（Expectation Step）和M步（Maximization Step）。

1. **E步（Expectation Step）**：在E步中，根据当前的参数估计值，计算隐变量的条件概率期望。
2. **M步（Maximization Step）**：在M步中，利用E步得到的隐变量的条件概率期望，更新模型的参数估计值。

通过反复迭代执行E步和M步，直至收敛，得到模型参数的估计值。EM算法的应用场景包括但不限于高斯混合模型、隐马尔可夫模型等。

### 朴素贝叶斯分类器基本原理

朴素贝叶斯分类器是一种基于贝叶斯定理和特征独立性假设的分类算法。其基本原理是利用已知类别的训练样本数据，学习每个类别下特征的条件概率分布，并通过贝叶斯定理计算给定特征条件下各个类别的后验概率，然后将样本分配到具有最高后验概率的类别中。

朴素贝叶斯分类器的"朴素"指的是它假设各个特征之间是独立的。虽然这个假设在实际数据中可能不成立，但朴素贝叶斯分类器在实践中通常表现出令人满意的性能，并且在文本分类等领域经常被广泛使用。

# 线性分类器

### 线性判别函数的基本概念

线性判别函数是一种用于将输入数据进行分类的线性模型。其基本概念是将输入数据投影到一个低维的线性空间，并在该空间中使用线性函数进行分类。线性判别函数的一般形式可以表示为：

$\[ f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b \]$

其中，\( \mathbf{w} \) 是权重向量，\( \mathbf{x} \) 是输入特征向量，\( b \) 是偏置项。

### 线性判别分析（Linear Discriminant Analysis, LDA）

线性判别分析是一种监督学习方法，用于降维和分类。它试图通过最大化类别之间的间距，同时最小化类别内的方差，来找到一个最优的投影方向。具体而言，LDA通过计算类别之间的类内散布矩阵和类间散布矩阵，然后找到使类间散布矩阵尽可能大、类内散布矩阵尽可能小的投影方向。在实践中，LDA通常用于降维和特征提取，也可以用于分类任务。

### 感知器基本原理与梯度下降方法

感知器是一种简单的二分类线性分类器，基于线性判别函数。其基本原理是利用训练数据集中的样本，通过迭代更新权重和偏置项，使得线性判别函数能够将正负样本分开。感知器的训练过程通常采用梯度下降方法，其中，通过计算损失函数对参数的梯度，并沿着梯度的方向更新参数值，直至达到收敛条件为止。

### 最优分类面与线性支持向量机（SVM）以及线性不可分支持向量机

线性支持向量机是一种用于二分类的监督学习算法，其基本思想是找到一个最优的超平面，能够将不同类别的数据分开，并且使得支持向量到超平面的距离最大化。当数据线性可分时，SVM通过求解凸优化问题来确定最优超平面。当数据线性不可分时，可以使用软间隔SVM，引入松弛变量来允许一些样本分类错误。

### 多类线性分类器的基本思路

对于多类分类问题，可以使用一对一（One-vs-One）或一对其他（One-vs-Rest）策略。一对一策略将每个类别与其他类别组合，构建多个二分类器进行分类，然后通过投票或其他规则决定最终分类结果。一对其他策略将每个类别作为一个类别，其余所有类别作为另一个类别，构建多个二分类器进行分类。无论采用哪种策略，都可以使用线性判别函数、支持向量机等线性模型进行分类。

# 神经网络和SVM

### 多层感知器神经网络（MLP）基本原理

多层感知器神经网络是一种常见的深度学习模型，由多个神经网络层组成，每一层包含多个神经元。它包括输入层、隐藏层和输出层。每个神经元接收来自上一层的输入，并将加权和传递给下一层的神经元。隐藏层的作用是提取输入数据的特征表示，输出层用于将这些特征表示映射到所需的输出。多层感知器通过反向传播算法来训练，该算法通过最小化损失函数来更新网络参数。

### 支持向量机（SVM）与核函数

支持向量机是一种用于二分类和多分类的监督学习模型。其基本原理是寻找一个最优的超平面，能够将不同类别的数据分开，并且使得支持向量到超平面的距离最大化。在线性可分情况下，可以直接使用线性支持向量机来求解。然而，当数据线性不可分时，可以引入核函数来将输入数据映射到更高维的特征空间，在该特征空间中寻找一个线性超平面来分隔数据。常用的核函数包括：
![[../../attachments/359e58ddff4e8ae8c7facb8af1eb5bea_MD5.png]]
1. **线性核函数**：$\( K(\mathbf{x}, \mathbf{x'}) = \mathbf{x}^T \mathbf{x'} \)$，适用于线性可分的情况。
2. **多项式核函数**：$\( K(\mathbf{x}, \mathbf{x'}) = (\gamma \mathbf{x}^T \mathbf{x'} + r)^d \)$，引入了多项式特征映射，可用于处理非线性问题。
3. **高斯核函数（径向基函数）**：$\( K(\mathbf{x}, \mathbf{x'}) = \exp(-\gamma ||\mathbf{x} - \mathbf{x'}||^2) \)$，将数据映射到无限维的特征空间，适用于各种非线性问题。应用最广泛，默认核函数。
4. **sigmoid核函数**：$\( K(\mathbf{x}, \mathbf{x'}) = \tanh(\gamma \mathbf{x}^T \mathbf{x'} + r) \)$，类似于神经网络中的激活函数，适用于一些特殊情况。

通过选择合适的核函数，支持向量机可以应用于各种复杂的分类问题，并且具有很强的泛化能力。

# 各类算法

1. **最近邻法（K-Nearest Neighbors，KNN）**：
    
    - 最近邻法是一种用于分类和回归的简单机器学习算法。对于分类任务，给定一个未标记的样本点，它将被标记为与其最接近的K个已标记样本中最常见的类别。对于回归任务，K个最近邻的输出的平均值被用作预测值。
2. **k近邻法（K-Nearest Neighbors，KNN）**：
    
    - k近邻法与最近邻法类似，但是不是只考虑一个最近的邻居，而是考虑最接近的k个邻居。在分类任务中，最常见的类别将被用于对未标记样本进行分类；在回归任务中，k个最近邻的输出的平均值被用作预测值。
3. **ID3方法**：
    
    - ID3是一种基于决策树的分类算法，它用于从给定的数据集中构建一个决策树。该算法根据信息增益选择最佳的特征来进行分裂，直到所有数据都被正确分类或者没有更多特征可用为止。ID3算法通常用于处理离散型数据。
4. **随机森林**：
    
    - 随机森林是一种集成学习方法，用于分类和回归任务。它基于决策树构建，但是在构建过程中引入了随机性。随机森林通过对数据随机抽样和对特征随机选择来生成多个决策树，并将它们的结果结合起来进行预测。这种结合的方式可以减少过拟合，并提高整体的预测性能。
5. **Adaboost算法**：
    
    - Adaboost（Adaptive Boosting）是一种集成学习方法，用于提高弱分类器的性能。该算法通过迭代训练一系列分类器，并根据每个分类器的性能调整样本权重，使得后续分类器更加关注被前面分类器错误分类的样本。最终的分类器是这些弱分类器的加权组合。Adaboost的目标是组合多个弱分类器以生成一个强分类器。

# 数据降维

1. **主成分分析（Principal Component Analysis，PCA）**：
    
    - PCA是一种常用的线性降维技术，它通过将原始数据投影到一个新的特征空间，使得投影后的特征具有最大的方差。PCA通过找到数据中最重要的方向（主成分）来实现降维，从而减少数据的维度并保留尽可能多的信息。
2. **K-L变换（Karhunen-Loève Transform，KLT）**：
    
    - K-L变换是PCA的数学基础之一，它通过计算协方差矩阵的特征向量和特征值来找到数据的主要方向。这些特征向量构成了新的基向量，而特征值则表示数据在每个基向量方向上的重要性。
3. **多维尺度分析（Multidimensional Scaling，MDS）**：
    
    - MDS是一种非线性降维技术，旨在保持数据点之间的距离关系。它通过将高维数据映射到低维空间，尽可能地保持数据点之间的距离来实现降维，以便可视化或分析数据的结构。
4. **局部线性嵌入（Locally Linear Embedding，LLE）**：
    
    - LLE是一种非线性降维技术，它试图在保持局部邻域关系的同时，将高维数据映射到低维空间。LLE将每个数据点表示为其最近邻的线性组合，并在低维空间中重建这些关系。
5. **t分布随机邻域嵌入（t-distributed Stochastic Neighbor Embedding，t-SNE）**：
    
    - t-SNE是一种非线性降维和可视化技术，它通过将高维数据映射到低维空间，使得在高维空间中相似的数据点在低维空间中仍然保持相似性。t-SNE在保留局部结构的同时，也能够很好地呈现全局结构。
6. **奇异值分解（Singular Value Decomposition，SVD）**：
    
    - SVD是一种用于矩阵分解的数学方法，它将一个矩阵分解为三个矩阵的乘积：一个正交矩阵、一个对角矩阵和一个正交矩阵的转置。SVD在数据降维、矩阵逆求解、特征提取等领域有广泛的应用。
7. **潜在语义分析（Latent Semantic Analysis，LSA）**：
    
    - LSA是一种用于文本挖掘和信息检索的技术，它通过奇异值分解来将文本文档表示为语义空间中的向量。LSA可以在文档之间进行语义相似性的比较，并用于搜索引擎、文本分类等任务。

# 聚类算法

1. **K均值聚类算法（K-Means Clustering）**：
    
    - K均值聚类是一种常用的聚类算法，它将数据分成K个不同的类别，使得每个数据点都属于离它最近的均值（质心）所代表的类别。算法的过程包括以下步骤：
        1. 随机选择K个初始质心（类别的中心点）。
        2. 将每个数据点分配到最近的质心所代表的类别。
        3. 根据分配的类别重新计算每个类别的质心。
        4. 重复步骤2和3，直到质心不再发生变化或者达到预定的迭代次数。
2. **分级聚类算法（Hierarchical Clustering）**：
    
    - 分级聚类是一种层次化的聚类方法，它不需要事先指定要聚类的数量。该算法的主要思想是逐步合并或者分裂数据点，直到所有数据点都在一个类别中或者达到预定的停止条件。分级聚类有两种主要的方法：
        - **凝聚式（Agglomerative）**：开始时，每个数据点都是一个类别，然后逐步合并最近的两个类别，直到达到停止条件。
        - **分裂式（Divisive）**：开始时，所有数据点都在一个类别中，然后逐步将类别分裂成更小的子类别，直到达到停止条件。

# DNN，CNN，RNN，GAN

1. **DNN（Deep Neural Network，深度神经网络）**：
    
    - DNN是一种基本的神经网络结构，由多个隐藏层组成，每个隐藏层包含多个神经元。每个隐藏层都通过非线性激活函数将其输入转换为输出。DNN通常用于解决复杂的模式识别和分类问题。
2. **CNN（Convolutional Neural Network，卷积神经网络）**：
    
    - CNN是一种专门设计用于处理具有网格结构数据（如图像）的神经网络结构。它通过在图像上应用卷积操作来提取特征，并通过池化操作减少特征图的尺寸。CNN在计算机视觉领域取得了巨大成功，用于图像分类、对象检测、图像分割等任务。
3. **RNN（Recurrent Neural Network，循环神经网络）**：
    
    - RNN是一种具有循环连接的神经网络结构，允许信息在网络内部进行持续传递。RNN在处理序列数据（如时间序列、自然语言文本等）时非常有效，因为它可以捕捉到序列中的时间依赖关系。然而，传统的RNN存在梯度消失或梯度爆炸的问题，因此出现了一些改进的结构，如长短期记忆网络（LSTM）和门控循环单元（GRU）。
4. **GAN（Generative Adversarial Network，生成对抗网络）**：
    
    - GAN是由两个神经网络组成的对抗性模型，一个是生成器（Generator），另一个是判别器（Discriminator）。生成器试图生成与真实数据相似的数据样本，而判别器则试图区分生成器生成的样本和真实数据之间的差异。这两个网络通过对抗训练的方式不断提升，最终使得生成器能够生成逼真的假样本。GAN在生成图像、视频、音频等方面有着广泛的应用。
