理论学习：DeepLearning
实践：DevInto DeepLearning

学习路径：
- 速览花书
- 看吴恩达DeepLearning和李沐的DevInto DeepLearningV2视频
- 做md笔记和思维导图


# 1、引言

### 环境配置见conda

## 各种机器学习问题

**监督学习**
回归，分类，标记问题，搜索，推荐系统，序列学习
**无监督学习**
聚类，主成分分析，因果关系，生成对抗网络（为我们提供⼀种合成数据的⽅法，甚⾄像图像和⾳频这样复杂的⾮结构化数据）
**强化学习**
强化学习的⽬标是产⽣⼀个好的策略（policy）。
![650](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/20230811001520.png)


## 深度学习的发展
	算⼒的增⻓速度已经超过了现有数据的增⻓速度，研究的关注点从线性模型和核方法来到了深度神经网络

下⾯列举了帮助研究⼈员在过去⼗年中取得巨⼤进步的想法
1. 新的容量控制方法
2. 注意力机制
3. 多阶段设计，例如存储器⽹络 (Sukhbaatar et al., 2015) 和编程器-解释器 (Reed and De Freitas,2015)
4. 生成对抗网络
5. 随机梯度下降
6. 并行计算
7. 一众深度学习框架，如torch，mxnet，tensorflowe


# 2、基础知识
## 数据处理

PyTorch 提供了许多用于数据处理的函数和功能，除了之前提到的数据加载器、数据集类、数据预处理和增强之外，还有一些其他常用的函数和工具。以下是一些常见的PyTorch数据处理函数和功能：

1. **数据切分：** 使用 `torch.utils.data.random_split` 函数可以将数据集切分成训练集、验证集和测试集。这对于模型训练和评估非常有用。

2. **数据变换：** 除了 `transforms` 中的预定义变换外，您还可以编写自定义的数据变换函数，以便在数据加载时进行更复杂的处理。

3. **样本权重：** 如果您的数据集中不同样本的重要性不同，可以使用 `torch.utils.data.WeightedRandomSampler` 来创建样本加权的数据加载器。

4. **数据持久化：** 使用 `torch.save` 和 `torch.load` 函数，可以将张量、模型和其他对象保存到磁盘上，并在需要时重新加载。

5. **多进程数据加载：** 使用 `num_workers` 参数，您可以指定数据加载器在多个进程中并行加载数据，以提高数据加载效率。

6. **自定义数据加载逻辑：** 您可以编写自定义的数据加载逻辑，例如从特定格式的数据文件中加载数据，然后将其转换为张量。

7. **数据处理流程封装：** 使用 `torch.utils.data.DataLoader` 和 `torch.utils.data.Dataset` 将数据加载、预处理和增强的逻辑封装成可重复使用的数据处理流程。

8. **数据统计和分析：** 您可以使用 NumPy 或其他统计工具对加载的数据进行统计分析，以便更好地了解数据分布。

9. **数据平衡：** 对于不平衡的数据集，可以采用各种方法来处理，如过采样、欠采样或使用专门的损失函数（如 Focal Loss）。

10. **数据可视化：** 使用库如 Matplotlib、TensorBoard 或其他工具，可以对数据进行可视化，包括样本展示、学习曲线绘制等。


### **自动广播机制**
1. 通过适当复制元素来扩展⼀个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对⽣成的数组执⾏按元素操作。
```PYTHON
A = torch.tensor([0, 1, 2, 3])
A / 2
```

### **节省内存**
```python
# 原地执行，确保不会销毁分配新内存，造成巨量性能浪费
x+=y,x[:]=x+y
```

### 数据预处理

处理缺失的数据，典型处理方法是插值和删除，插值可以是插入平均值
如果是类别值或者离散值，可以将NaN视为一个类别


## 线性代数

**标量**
严格来说，仅包含一个数值的变量被称为*标量*（scalar）。
例如，北京的温度为$52^{\circ}F$（华氏度，除摄氏度外的另一种温度计量单位）。
**向量**
在数学中，向量$\mathbf{x}$可以写为：
$$\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix},$$
**矩阵**
数学表示法使用$\mathbf{A} \in \mathbb{R}^{m \times n}$
来表示矩阵$\mathbf{A}$，其由$m$行和$n$列的实值标量组成。
我们可以将任意矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$视为一个表格，
其中每个元素$a_{ij}$属于第$i$行第$j$列：
$$\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.$$

**转置**
当我们交换矩阵的行和列时，结果称为矩阵的*转置*（transpose）。
通常用$\mathbf{a}^\top$来表示矩阵的转置，如果$\mathbf{B}=\mathbf{A}^\top$，
则对于任意$i$和$j$，都有$b_{ij}=a_{ji}$。
因此，转置是一个形状为$n \times m$的矩阵：
$$
\mathbf{A}^\top =

\begin{bmatrix}

    a_{11} & a_{21} & \dots  & a_{m1} \\

    a_{12} & a_{22} & \dots  & a_{m2} \\

    \vdots & \vdots & \ddots  & \vdots \\

    a_{1n} & a_{2n} & \dots  & a_{mn}

\end{bmatrix}.
$$

**Hadamard积**
对于矩阵$\mathbf{B} \in \mathbb{R}^{m \times n}$，
其中第$i$行和第$j$列的元素是$b_{ij}$。
矩阵$\mathbf{A}$和$\mathbf{B}$的Hadamard积为：

$$
\mathbf{A} \odot \mathbf{B} =

\begin{bmatrix}

    a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\

    a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\

    \vdots & \vdots & \ddots & \vdots \\

    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}

\end{bmatrix}.
$$

**降维**
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。
我们还可以**指定张量沿哪一个轴来通过求和降低维度**。
以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。
由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。
点积dot
给定两个向量$\mathbf{x},\mathbf{y}\in\mathbb{R}^d$，它们的*点积*（dot product）$\mathbf{x}^\top\mathbf{y}$（或$\langle\mathbf{x},\mathbf{y}\rangle$）是相同位置的按元素**乘积的和**：$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$。

**向量积**
$$
\mathbf{A}\mathbf{x}

= \begin{bmatrix}

\mathbf{a}^\top_{1} \\

\mathbf{a}^\top_{2} \\

\vdots \\

\mathbf{a}^\top_m \\

\end{bmatrix}\mathbf{x}

= \begin{bmatrix}

 \mathbf{a}^\top_{1} \mathbf{x}  \\

 \mathbf{a}^\top_{2} \mathbf{x} \\

\vdots\\

 \mathbf{a}^\top_{m} \mathbf{x}\\

\end{bmatrix}.
$$

**矩阵乘法**
假设有两个矩阵$\mathbf{A} \in \mathbb{R}^{n \times k}$和$\mathbf{B} \in \mathbb{R}^{k \times m}$：

$$\mathbf{A}=\begin{bmatrix}

 a_{11} & a_{12} & \cdots & a_{1k} \\

 a_{21} & a_{22} & \cdots & a_{2k} \\

\vdots & \vdots & \ddots & \vdots \\

 a_{n1} & a_{n2} & \cdots & a_{nk} \\

\end{bmatrix},\quad

\mathbf{B}=\begin{bmatrix}

 b_{11} & b_{12} & \cdots & b_{1m} \\

 b_{21} & b_{22} & \cdots & b_{2m} \\

\vdots & \vdots & \ddots & \vdots \\

 b_{k1} & b_{k2} & \cdots & b_{km} \\

\end{bmatrix}.$$

用行向量$\mathbf{a}^\top_{i} \in \mathbb{R}^k$表示矩阵$\mathbf{A}$的第$i$行，并让列向量$\mathbf{b}_{j} \in \mathbb{R}^k$作为矩阵$\mathbf{B}$的第$j$列。要生成矩阵积$\mathbf{C} = \mathbf{A}\mathbf{B}$，最简单的方法是考虑$\mathbf{A}$的行向量和$\mathbf{B}$的列向量:
$$\mathbf{A}=

\begin{bmatrix}

\mathbf{a}^\top_{1} \\

\mathbf{a}^\top_{2} \\

\vdots \\

\mathbf{a}^\top_n \\

\end{bmatrix},

\quad \mathbf{B}=\begin{bmatrix}

 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\

\end{bmatrix}.

$$
当我们简单地将每个元素$c_{ij}$计算为点积$\mathbf{a}^\top_i \mathbf{b}_j$:
$$\mathbf{C} = \mathbf{AB} = \begin{bmatrix}

\mathbf{a}^\top_{1} \\

\mathbf{a}^\top_{2} \\

\vdots \\

\mathbf{a}^\top_n \\

\end{bmatrix}

\begin{bmatrix}

 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\

\end{bmatrix}

= \begin{bmatrix}

\mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\

 \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\

 \vdots & \vdots & \ddots &\vdots\\

\mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m

\end{bmatrix}.
$$
我们可以将矩阵-矩阵乘法$\mathbf{AB}$看作简单地执行**m次矩阵-向量积**，并将结果拼接在一起，形成一个$n \times m$矩阵。
这里的`A`是一个5行4列的矩阵，`B`是一个4行3列的矩阵。
两者相乘后，我们得到了一个5行3列的矩阵。

**范数norm**
线性代数中最有用的一些运算符是*范数*（norm）。非正式地说，向量的*范数*是表示一个向量有多大。这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。

在线性代数中，向量范数是将向量映射到标量的函数$f$。给定任意向量$\mathbf{x}$，向量范数要满足一些属性。第一个性质是：如果我们按常数因子$\alpha$缩放向量的所有元素，其范数也会按相同常数因子的*绝对值*缩放：
$$f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).$$
第二个性质是熟悉的三角不等式:
$$f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).$$
第三个性质简单地说范数必须是非负的:
$$f(\mathbf{x}) \geq 0.$$

这是有道理的。因为在大多数情况下，任何东西的最小的*大小*是0。最后一个性质要求范数最小为0，当且仅当向量全由0组成。
$$\forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.$$
范数听起来很像距离的度量。欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。事实上，欧几里得距离是一个$L_2$范数：
假设$n$维向量$\mathbf{x}$中的元素是$x_1,\ldots,x_n$，**其$L_2$*范数*是向量元素平方和的平方根：**
$$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},$$
其中，在$L_2$范数中常常省略下标$2$，也就是说$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$。
深度学习中更经常地使用$L_2$范数的平方，也会**经常遇到$L_1$范数，它表示为向量元素的绝对值之和：**
$$\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.$$
与$L_2$范数相比，$L_1$范数受异常值的影响较小。为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来。
$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：
$$\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.$$
### 优化问题
在深度学习中，我们经常试图解决**优化问题**：
最大化分配给观测数据的概率;最小化预测和真实观测之间的距离。
用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。
目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。
### 矩阵的类型
方阵，正交矩阵，置换矩阵（正交），正定矩阵， 二次型
### 特征向量
不被矩阵改变的向量，对策矩阵总能找到特征向量⋋
$$\mathbf{AX}=\mathbf{⋋X}$$

### 小结
* 标量、向量、矩阵和张量是线性代数中的基本数学对象。
* 向量泛化自标量，矩阵泛化自向量。
* 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。
* 一个张量可以通过`sum`和`mean`沿指定的轴降低维度。
* 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。
* 在深度学习中，我们经常使用范数，如$L_1$范数、$L_2$范数和Frobenius范数。
* 我们可以对标量、向量、矩阵和张量执行各种操作。



## 微分求导

<img src="https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/20230813190426.png" alt="image.png" style="zoom: 33%;" />

其中<X,W>是向量X和W的内积（点积），是一个标量。
而X必定为行向量，W必定为列向量
∂(<X, W>) / ∂W = ∂(<X, W>) / ∂W1, ∂(<X, W>) / ∂W2, ..., ∂(<X, W>) / ∂Wn = $\mathbf{X}^\top$
由于W为列向量，所以最终结果是$\mathbf{X}^\top$

(**如果$f$的*导数*存在，这个极限被定义为**)
$$f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.$$

## 自动求导
自动求导计算一个函数在指定值上的导数，它与符号求导，数值求导并不相同
<img src="https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/20230813191131.png" alt="image.png" style="zoom: 33%;" />


1. 动态图：PyTorch使用动态图来记录计算图。这意味着在计算过程中，每个操作都会被立即执行，并且计算图是根据实际的代码执行顺序动态构建的。这种动态图的设计使得PyTorch非常灵活，允许用户根据需要进行动态的模型构建和调整。

2. 反向自动求导：PyTorch使用反向自动求导（Reverse Mode Automatic Differentiation）来计算张量的导数。在计算图构建完成后，用户只需要调用`backward()`方法，PyTorch就会自动计算所有需要求导的张量的导数。这种反向自动求导的方式使得用户不需要手动计算导数，大大简化了求导的过程。

反向求导就是朴素的人解题思维，反向求导优势在于不需要存储任何中间结果，内存复杂度低，但两者的代价通常差不多，因为反向求导每轮都需要扫描梯度。

<img src="https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/20230813191317.png" alt="image.png" style="zoom: 33%;" />

<img src="https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/20230813191451.png" alt="image.png" style="zoom:33%;" />
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1691926511000xcm0va.png)



3. `torch.Tensor`类：PyTorch中的`torch.Tensor`类是自动求导的关键类。用户可以通过设置`requires_grad`属性来指定某个张量是否需要求导。当一个张量需要求导时，PyTorch会自动追踪其在计算图中的操作，并在调用`backward()`方法时计算该张量的导数。

4. `torch.autograd`模块：`torch.autograd`模块是PyTorch中用于自动求导的核心模块。它提供了一些函数和类，用于构建计算图、计算导数和管理计算图中的张量。例如，`torch.autograd.Variable`类可以用来包装张量，使其具有自动求导的功能。

5. 计算图的优化：PyTorch的自动求导机制还支持计算图的优化，以提高计算效率。例如，当某个张量的导数已经计算过一次后，PyTorch会缓存导数的值，以避免重复计算。此外，PyTorch还提供了一些优化函数，如`torch.autograd.grad()`和`torch.autograd.backward()`，用于更灵活地管理计算图和导数的计算。


y = x * x
- y.sum().backward() 对标量求导
- y.backward() 对向量求导
二者的区别？
**一般而言求导的目的是获得偏导数之和，所以一般先sum()**



## 概率论

### 概率论公理
在处理骰子掷出时，我们将集合$\mathcal{S} = \{1, 2, 3, 4, 5, 6\}$
称为*样本空间*（sample space）或*结果空间*（outcome space），
其中每个元素都是*结果*（outcome）。
*事件*（event）是一组给定样本空间的随机结果。

*概率*（probability）可以被认为是将集合映射到真实值的函数。
在给定的样本空间$\mathcal{S}$中，事件$\mathcal{A}$的概率，
表示为$P(\mathcal{A})$，满足以下属性：
* 对于任意事件$\mathcal{A}$，其概率从不会是负数，即$P(\mathcal{A}) \geq 0$；
* 整个样本空间的概率为$1$，即$P(\mathcal{S}) = 1$；
* 对于*互斥*（mutually exclusive）事件（对于所有$i \neq j$都有$\mathcal{A}_i \cap \mathcal{A}_j = \emptyset$）的任意一个可数序列$\mathcal{A}_1, \mathcal{A}_2, \ldots$，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即$P(\bigcup_{i=1}^{\infty} \mathcal{A}_i) = \sum_{i=1}^{\infty} P(\mathcal{A}_i)$。

### 随机变量

在我们掷骰子的随机实验中，我们引入了*随机变量*（random variable）的概念。
随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。
考虑一个随机变量$X$，其值在掷骰子的样本空间$\mathcal{S}=\{1,2,3,4,5,6\}$中。
我们可以简单用$P(a)$表示随机变量取值$a$的概率。

由于概率论中的事件是来自样本空间的一组结果，因此我们可以为随机变量指定值的可取范围。
例如，$P(1 \leq X \leq 3)$表示事件$\{1 \leq X \leq 3\}$，
即$\{X = 1, 2, \text{or}, 3\}$的概率。
等价地，$P(1 \leq X \leq 3)$表示随机变量$X$从$\{1, 2, 3\}$中取值的概率。

  
请注意，*离散*（discrete）随机变量（如骰子的每一面）
和*连续*（continuous）随机变量（如人的体重和身高）之间存在微妙的区别。
现实生活中，测量两个人是否具有完全相同的身高没有太大意义。
如果我们进行足够精确的测量，最终会发现这个星球上没有两个人具有完全相同的身高。
在这种情况下，询问某人的身高是否落入给定的区间，比如是否在1.79米和1.81米之间更有意义。
在这些情况下，我们将这个看到某个数值的可能性量化为*密度*（density）。
高度恰好为1.80米的概率为0，但密度不是0。
在任何两个不同高度之间的区间，我们都有非零的概率。
在本节的其余部分中，我们将考虑离散空间中的概率。

### 处理多个随机变量
很多时候，我们会考虑多个随机变量。比如，我们可能需要对疾病和症状之间的关系进行建模。给定一个疾病和一个症状，比如“流感”和“咳嗽”，以某个概率存在或不存在于某个患者身上。我们需要估计这些概率以及概率之间的关系，以便我们可以运用我们的推断来实现更好的医疗服务。
再举一个更复杂的例子：图像包含数百万像素，因此有数百万个随机变量。在许多情况下，图像会附带一个*标签*（label），标识图像中的对象。我们也可以将标签视为一个随机变量。我们甚至可以将所有元数据视为随机变量，例如位置、时间、光圈、焦距、ISO、对焦距离和相机类型。所有这些都是联合发生的随机变量。当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。

**联合概率**
第一个被称为*联合概率*（joint probability）$P(A=a,B=b)$。
给定任意值$a$和$b$，联合概率可以回答：$A=a$和$B=b$同时满足的概率是多少？
请注意，对于任何$a$和$b$的取值，$P(A = a, B=b) \leq P(A=a)$。
这点是确定的，因为要同时发生$A=a$和$B=b$，$A=a$就必须发生，$B=b$也必须发生（反之亦然）。因此，$A=a$和$B=b$同时发生的可能性不大于$A=a$或是$B=b$单独发生的可能性。
**条件概率**
联合概率的不等式带给我们一个有趣的比率：
$0 \leq \frac{P(A=a, B=b)}{P(A=a)} \leq 1$。
我们称这个比率为*条件概率*（conditional probability），并用$P(B=b \mid A=a)$表示它：它是$B=b$的概率，前提是$A=a$已发生。
**贝叶斯定理**
使用条件概率的定义，我们可以得出统计学中最有用的方程之一：*Bayes定理*（Bayes' theorem）。
根据*乘法法则*（multiplication rule ）可得到$P(A, B) = P(B \mid A) P(A)$。
根据对称性，可得到$P(A, B) = P(A \mid B) P(B)$。
假设$P(B)>0$，求解其中一个条件变量，我们得到$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.$
请注意，这里我们使用紧凑的表示法：
其中$P(A, B)$是一个*联合分布*（joint distribution），$P(A \mid B)$是一个*条件分布*（conditional distribution）。
这种分布可以在给定值$A = a, B=b$上进行求值。
**边际化**
为了能进行事件概率求和，我们需要*求和法则*（sum rule），即$B$的概率相当于计算$A$的所有可能选择，并将所有选择的联合概率聚合在一起：
$P(B) = \sum_{A} P(A, B),$
这也称为*边际化*（marginalization）。
边际化结果的概率或分布称为*边际概率*（marginal probability）或*边际分布*（marginal distribution）。
**独立性**
另一个有用属性是*依赖*（dependence）与*独立*（independence）。
如果两个随机变量$A$和$B$是独立的，意味着事件$A$的发生跟$B$事件的发生无关。
在这种情况下，统计学家通常将这一点表述为$A \perp  B$。
根据贝叶斯定理，马上就能同样得到$P(A \mid B) = P(A)$。
在所有其他情况下，我们称$A$和$B$依赖。
比如，两次连续抛出一个骰子的事件是相互独立的。
相比之下，灯开关的位置和房间的亮度并不是（因为可能存在灯泡坏掉、电源故障，或者开关故障）。
由于$P(A \mid B) = \frac{P(A, B)}{P(B)} = P(A)$等价于$P(A, B) = P(A)P(B)$，因此两个随机变量是独立的，当且仅当两个随机变量的联合分布是其各自分布的乘积。
同样地，给定另一个随机变量$C$时，两个随机变量$A$和$B$是*条件独立的*（conditionally independent），当且仅当$P(A, B \mid C) = P(A \mid C)P(B \mid C)$。
这个情况表示为$A \perp B \mid C$。


# 线性神经网络
### 单层神经网络
正态分布（高斯分布）
$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$
极大似然估计
$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).$$


y是真实值，x是输入参数，w是权重
$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b.$$
损失函数
$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$
代入$\hat{y}$累计损失
$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$
最小化损失函数
$$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$

对w求偏导，得到w的导数是一个凸函数
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692016979000666w3a.png)
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16920169900000esrgf.png)
解析解
$${W}^{*}=(X^{T}X)^{-1}X^{T}Y$$
推导过程
![gh|300](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16920168380000bk6lj.png)

### 随机梯度下降
更新梯度过程
$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$
详细步骤
$$\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}$$

对损失函数求导
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16920169420004f77pe.png)



### 梯度下降参数

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692017742000ifojqj.png)

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692017834000h3h82h.png)


### 总结
- 梯度下降通过不断沿着反梯度方向更新参数求解
- 小批量随机梯度下降是深度学习默认的求解算法
- 两个重要的超参数是批量大小和学习率




## SoftMax
softmax是将数据转化为易读的概率，SoftMax的真实结果集只有一项为非0，即onehot编码
$$
\begin{align} \\

\mathbf{y} =(0, 0, 1)，&\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}  \\

\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad &\text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)} \tag{1}

\end{align}
$$

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692163380000ur8ny5.png)

### 损失函数
#### 对数似然
$o_{j}$的最大值和$y_{j}$的最大值是同一项
$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$
之所以是$\log y_{y}$，是因为只取$y_{i}$的正确项进行比较，$y_{y}$代表唯一的正确项
根据**交叉熵理论公式**可得**损失函数**，log的底数为e
$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
由于$y_{j}$是独热编码只有一个值为1，所以$y_{j}=1$时损失函数通常为
$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \log \hat{y}_j.$$
$$
\begin{aligned}

l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\

&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\

&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.

\end{aligned}
$$
推导得出$o_{j}$的偏导数，即**梯度**
$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) =\frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j= \hat{y}_j - y_j
$$

### 小结
• softmax运算获取一个向量并将其映射为概率。
• softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。
• 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。




# 多层感知机

多层感知机（Multilayer Perceptron，简称 MLP）是一种常用的前馈人工神经网络模型。它由多个神经网络层组成，每个层由多个神经元（或称为节点）组成。每个神经元接收来自前一层神经元的输入，并通过激活函数对输入进行非线性变换，然后将输出传递给下一层神经元。
MLP 的核心思想是通过多个层次的非线性变换来学习输入数据的**复杂特征**表示。它具有较强的表达能力和适应性，可以应用于分类、回归和其他机器学习任务。MLP 的训练通常使用反向传播算法，通过最小化损失函数来优化网络参数。

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1695700206000dv0cih.png)

MLP的基本结构包括输入层、隐藏层和输出层。
1. 输入层接收外部输入数据
2. 隐藏层通过一系列的**线性变换和非线性激活函数**对输入进行处理
3. 输出层产生最终的预测结果
具有全连接层的多层感知机的参数开销可能会高得令人望而却步

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1695302305000izieh4.png)

### 加入隐藏层
线性模型意味着单调的假设，无法预测复杂情况，在网络中加入一个或多个隐藏层可以克服线性模型的限制，从而处理更普遍的

### 线性隐藏层毫无作用
注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。  
**可我们能从中得到什么好处呢？我们没有好处！**  
原因很简单：上面的隐藏单元由输入的仿射函数给出，  而输出（softmax操作前）只是隐藏单元的仿射函数。  仿射函数的仿射函数本身就是仿射函数，  但是我们之前的线性模型已经能够表示任何仿射函数。

### 非线性激活函数
没有非线性激活函数，多层感知机会退化成线性模型

ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。
$$\operatorname{ReLU}(x) = \max(x, 0) \tag{1} $$
<div style="text-align: center;"><img src="https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/169530127200069nudf.png" alt="gh" style="zoom: 50%;" />
</div>

sigmoid函数将输入变换为区间(0, 1)上的输出(sigmoid可以视为softmax的特例），大部分时间使用计算代价更低的ReLU
$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)} \tag{2}$$
<div style="text-align: center;">
<img src="https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1695301258000csvhdo.png" alt="gh" style="zoom:50%;" />
</div>
与sigmoid函数类似，tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上
$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} \tag{3}$$
<div style="text-align: center;">
<img src="https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1695301219000gbfltn.png" alt="gh" style="zoom:50%;" />
</div>

### 小结
- 多层感知机使用隐藏层和激活函数来得到非线性模型
- 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数
- 使用Softmax处理多类分类问题
- 超参数数为隐藏层数，和各个隐藏层大小


# 模型选择

### 数据集

- 训练数据集：训练模型参数
- 验证数据集：调整模型超参数
- 测试数据集：仅使用一次的测试数据
- 通常使用K则交叉检验充分利用数据集

### 拟合

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16958183120007z70f4.png)

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1695818724000if4kra.png)

### VC维
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16958192130008ws7xx.png)
- 可以提供一个模型为什么好的理论依据
- 可以衡量训练误差和泛化误差的间隔
- 深度学习中不常用

### K折交叉检验

```python
from sklearn.pipeline import Pipeline  
from sklearn.model_selection import KFold, cross_val_score  
from sklearn.preprocessing import StandardScaler  
from sklearn.svm import SVC  
import joblib  
  
# 创建一个Pipeline，包括数据预处理和分类器  
pipeline = Pipeline([  
    ('scaler', StandardScaler()),  # 特征缩放  
    ('classifier', SVC())  # 分类器，这里使用SVM作为示例  
])  
  
# 创建一个KFold对象，指定k的值和随机种子（可选）  
kfold = KFold(n_splits=5, random_state=42)  
  
# 创建一个空的模型列表  
models = []  
  
# 使用cross_val_score进行k折交叉验证  
for train_index, val_index in kfold.split(X):  
    X_train, X_val = X[train_index], X[val_index]  
    y_train, y_val = y[train_index], y[val_index]  
      
    # 训练模型  
    model = pipeline.fit(X_train, y_train)  
      
    # 保存模型  
    models.append(model)  
    joblib.dump(model, f"model_fold_{len(models)}.joblib")  
  
    # 在验证集上评估模型性能  
    score = model.score(X_val, y_val)  
    print(f"Fold {len(models)} Accuracy: {score}")  
  
# 加载最终的测试集数据  
X_test = np.array([[...], [...], ...])  # 最终测试集的特征矩阵  
y_test = np.array([..., ..., ...])  # 最终测试集的目标变量  
  
# 创建一个空的预测结果列表  
predictions = []  
  
# 使用训练好的模型对最终测试集进行预测  
for model in models:  
    y_pred = model.predict(X_test)  # 对最终测试集进行预测  
    predictions.append(y_pred)  # 将预测结果添加到列表中  
  
# 对预测结果进行投票或求平均  
ensemble_predictions = np.mean(predictions, axis=0)  # 取预测结果的平均值  
  
# 对于分类任务，可以使用投票来确定最终预测结果  
final_predictions = np.round(ensemble_predictions)  # 四舍五入为最终的预测结果  
  
# 计算预测准确率或其他性能指标  
accuracy = np.mean(final_predictions == y_test)  # 计算准确率  
  
# 输出预测准确率  
print("Final Accuracy:", accuracy)
```

### 总结

- 模型容量需要匹配数据复杂度，否则可能欠拟合和过拟合
- 统计机器学习提供数学工具来衡量模型复杂度
- 实际中一般靠观察训练误差和验证误差

# 正则化
### 权重衰退

一种简单的方法是通过线性函数$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$中的权重向量的某个范数来度量其复杂性，例如$\| \mathbf{w} \|^2$。要保证权重向量比较小，最常用方法是将其范数作为惩罚项加到最小化损失的问题中。将原来的训练目标*最小化训练标签上的预测损失*，调整为*最小化预测损失和惩罚项之和*。

**惩罚权重：** Loss函数的基础上再对w加一个L2范数，可限制w的范围
$$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$
在损失函数上额外加一项范数
$$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,$$
L2正则化小批量随机梯度下降
$$

\begin{aligned}

\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).

\end{aligned}

$$



### 丢弃法（Dropout）

暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。这种方法之所以被称为*暂退法*，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。

**以一种*无偏向*（unbiased）的方式注入噪声。**
在固定住其他层时，每一层的期望值等于没有噪音时的值。将高斯噪声添加到线性模型的输入中。在每次训练迭代中，他将从均值为零的分布$\epsilon \sim \mathcal{N}(0,\sigma^2)$采样噪声添加到输入$\mathbf{x}$，从而产生扰动点$\mathbf{x}' = \mathbf{x} + \epsilon$，预期是$E[\mathbf{x}'] = \mathbf{x}$。

**p为超参数，表示丢弃的概率**
在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。
换言之，每个中间活性值$h$以*暂退概率*$p$由随机变量$h'$替换，如下所示：

$$

\begin{aligned}

h' =

\begin{cases}

    0 & \text{ 概率为 } p \\

    \frac{h}{1-p} & \text{ 其他情况}

\end{cases}

\end{aligned}

$$
根据此模型的设计，其期望值保持不变，即$E[h'] = h$。


* 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。
* 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。
* 暂退法将活性值$h$替换为具有期望值$h$的随机变量。
* 暂退法仅在训练期间使用。


### 传播算法
推理是前向传播，训练为反向？
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/169745473000001cc8j.png)


• 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。
• 反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。
• 在训练深度学习模型时，前向传播和反向传播是相互依赖的。
• 训练比预测需要更多的内存，因为需要存储梯度更新参数。

# 模型初始化和梯度

### 梯度爆炸和消失

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16974549850004b0g6g.png)

### Xavier初始化

让我们看看某些*没有非线性*的全连接层输出（例如，隐藏变量）$o_{i}$的尺度分布。
对于该层$n_\mathrm{in}$输入$x_j$及其相关权重$w_{ij}$，输出由下式给出
$$o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j.$$
计算$o_i$的平均值和方差：
$$

\begin{aligned}

    E[o_i] & = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\

    \mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\

        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\

        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\

        & = n_\mathrm{in} \sigma^2 \gamma^2.

\end{aligned}

$$


保持方差不变的一种方法是设置$n_\mathrm{in} \sigma^2 = 1$。
但考虑反向和正向传播同时存在，不可能输入输出同时为1，所以选择折中方案
$$

\begin{aligned}

\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }

\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.

\end{aligned}

$$

这就是现在标准且实用的*Xavier初始化*的基础，通常，Xavier初始化从均值为零，方差

$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$

注意均匀分布$U(-a, a)$的方差为$\frac{a^2}{3}$。
将$\frac{a^2}{3}$代入到$\sigma^2$的条件中，将得到初始化值域：
$$U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right).$$

