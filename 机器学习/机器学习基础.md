
# 绪论

## 0.1. 机器学习的类别
1. 监督学习
	1. 线性回归
	2. 分类
2. 无监督学习
	1. 聚类算法【谷歌新闻】
3. 强化学习



# 线性回归模型Week1

$$Y_{i}=WX_{i}+b$$
权重W，偏移b

## 0.1. Cost代价函数

$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$
w权重，b偏移
$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\tag{2}$$
m训练集的数量，$f_{w,b}(x^{(i)})$是预测值， $y^{(i)})$是真实值

## 0.2. Cost偏导数梯度
$$
\begin{align} \\

\frac{\partial J(w,b)}{\partial w}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \tag{4}\\

  \frac{\partial J(w,b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \tag{5}\\ \\

\end{align}

$$

## 0.3. 梯度下降
[[机器学习基础#梯度下降]]

分别对两个参数进行梯度下降 
$$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \\

\;  w &= w -  \alpha \frac{\partial J(w,b)}{\partial w}  \; \\

 b &= b -  \alpha \frac{\partial J(w,b)}{\partial b} \\
  \alpha &=学习率
 \tag{3} \; \\ \rbrace

\end{align*}$$


必须同步更新所有参数
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/image.png)

为什么梯度会下降
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692345352000qiwlt9.png)

达到极小值处后收敛，不再发生变化
梯度下降的精髓，即使学习率不变，到达极小值附近时步长也会自动变小（斜率下降）
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16923458210009rylvf.png)


梯度下降的过程
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692346999000kbbn2g.png)


### 0.4. 多元线性回归矢量化

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16924332260002pjaor.png)

效率大幅提示

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692433550000dycptd.png)
### 0.5. 正规则方程
仅限用于线性回归模型的求解方法，当数据集较大时不适合使用。

# 多维特征Week2

## 1. 特征工程

### 1.1. 特征缩放

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692435350000ja7nlk.png)
### 1.2. 均值归一化
$$x_{i}=\frac{{x_{i}-u_{i}}}{max-min} \tag{Mean normalization}$$
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/169243555000085pa68.png)

### 1.3. Z-score正态分布
$$
\begin{align} \\

x^{(i)}_j &= \dfrac{x^{(i)}_j - \mu_j}{\sigma_j} \tag{Z-Score} \\ \\

\mu_j &= \frac{1}{m} \sum_{i=0}^{m-1} x^{(i)}_j \tag{1}\\

\sigma^2_j &= \frac{1}{m} \sum_{i=0}^{m-1} (x^{(i)}_j - \mu_j)^2  \\
 \tag{2} \\

 {\sigma_j} &=标准差，\mu_{j}=均值 
\end{align}

$$

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692435984000dc6teh.png)

特征缩放可以显著提升梯度下降速度
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16924363890006v7uzn.png)

## 2. 梯度下降收敛

### 2.1. 学习率
当**Cost值出现波动时**， 要考虑学习率是否过大，这时可以取一个极其小的学习率来验证，如果Cost还是没有稳定下降，那可能其他代码有问题。
如果**Cost缓慢下降**，那可能是学习率太小了。


![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692437017000x5pe8b.png)


### 2.2. 多项式回归功能
通常选择开根号,多项式回归的函数可以是输入项X的线性函数，也可以是非线性关系，这可能有利于更好的拟合

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/1692437693000wguvol.png)

例子
进行三次方的多项式回归，曲线被非常好的拟合了，但因为各个特征数值差异过大，速度比较慢
![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16925765870000m3tnc.png)
进行Z-score处理，拟合速度和Cost精度出现了飞跃性增长

![gh](https://cdn.jsdelivr.net/gh/narugakuru/images@master/img/16925766610001yhhzg.png)




# 逻辑回归Week3













# 符号
## 1. 数字
* $x$：标量
* $\mathbf{x}$：向量
* $\mathbf{X}$：矩阵
* $\mathsf{X}$：张量
* $\mathbf{I}$：单位矩阵
* $x_i$, $[\mathbf{x}]_i$：向量$\mathbf{x}$第$i$个元素
* $x_{ij}$, $[\mathbf{X}]_{ij}$：矩阵$\mathbf{X}$第$i$行第$j$列的元素

## 2. 集合论
* $\mathcal{X}$: 集合
* $\mathbb{Z}$: 整数集合
* $\mathbb{R}$: 实数集合
* $\mathbb{R}^n$: $n$维实数向量集合
* $\mathbb{R}^{a\times b}$: 包含$a$行和$b$列的实数矩阵集合
* $\mathcal{A}\cup\mathcal{B}$: 集合$\mathcal{A}$和$\mathcal{B}$的并集
* $\mathcal{A}\cap\mathcal{B}$：集合$\mathcal{A}$和$\mathcal{B}$的交集
* $\mathcal{A}\setminus\mathcal{B}$：集合$\mathcal{A}$与集合$\mathcal{B}$相减，$\mathcal{B}$关于$\mathcal{A}$的相对补集

## 3. 函数和运算符
* $f(\cdot)$：函数
* $\log(\cdot)$：自然对数
* $\exp(\cdot)$: 指数函数
* $\mathbf{1}_\mathcal{X}$: 指示函数
* $\mathbf{(\cdot)}^\top$: 向量或矩阵的转置
* $\mathbf{X}^{-1}$: 矩阵的逆
* $\odot$: 按元素相乘
* $[\cdot, \cdot]$：连结
* $\lvert \mathcal{X} \rvert$：集合的基数
* $\|\cdot\|_p$: ：$L_p$ 正则
* $\|\cdot\|$: $L_2$ 正则
* $\langle \mathbf{x}, \mathbf{y} \rangle$：向量$\mathbf{x}$和$\mathbf{y}$的点积
* $\sum$: 连加
* $\prod$: 连乘
* $\stackrel{\mathrm{def}}{=}$：定义

## 4. 微积分
* $\frac{dy}{dx}$：$y$关于$x$的导数
* $\frac{\partial y}{\partial x}$：$y$关于$x$的偏导数
* $\nabla_{\mathbf{x}} y$：$y$关于$\mathbf{x}$的梯度
* $\int_a^b f(x) \;dx$: $f$在$a$到$b$区间上关于$x$的定积分
* $\int f(x) \;dx$: $f$关于$x$的不定积分

## 5. 概率与信息论
* $P(\cdot)$：概率分布
* $z \sim P$: 随机变量$z$具有概率分布$P$
* $P(X \mid Y)$：$X\mid Y$的条件概率
* $p(x)$: 概率密度函数
* ${E}_{x} [f(x)]$: 函数$f$对$x$的数学期望
* $X \perp Y$: 随机变量$X$和$Y$是独立的
* $X \perp Y \mid Z$: 随机变量$X$和$Y$在给定随机变量$Z$的条件下是独立的
* $\mathrm{Var}(X)$: 随机变量$X$的方差
* $\sigma_X$: 随机变量$X$的标准差
* $\mathrm{Cov}(X, Y)$: 随机变量$X$和$Y$的协方差
* $\rho(X, Y)$: 随机变量$X$和$Y$的相关性
* $H(X)$: 随机变量$X$的熵
* $D_{\mathrm{KL}}(P\|Q)$: $P$和$Q$的KL-散度

## 6. 复杂度
* $\mathcal{O}$：大O标记

[Discussions](https://discuss.d2l.ai/t/2089)