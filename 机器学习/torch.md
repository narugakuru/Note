
# optim
`torch.optim`是PyTorch中用于优化算法的模块。它提供了各种常用的优化算法，如随机梯度下降（SGD）、Adam、Adagrad等，用于训练神经网络模型。

`torch.optim`模块的核心是`Optimizer`类，它是一个抽象基类，定义了优化算法的通用接口。通过创建`Optimizer`类的实例，可以将其与模型的参数进行关联，并使用指定的优化算法来更新参数。

在优化器的创建中，我们使用了`SGD`优化算法，并将模型的参数传递给优化器。通过调用`optimizer.step()`方法，可以根据损失函数的梯度更新模型的参数。
# nn.Sequential
`torch.nn.Sequential`是PyTorch中的一个函数库，用于构建神经网络模型。它提供了一种方便的方式来按顺序组合多个神经网络层。

使用`torch.nn.Sequential`，您可以将多个层按照顺序添加到模型中，而无需手动定义每个层的输入和输出。这样可以简化神经网络的构建过程，并提高代码的可读性。

`torch.nn.Sequential`类还提供了一个名为`apply`的方法，用于对模型的每个子模块应用自定义函数。`apply`方法接受一个函数作为参数，并将该函数应用于模型的每个子模块。