
# 概念区分

- 机器学习是一种实现人工智能的方法
- 深度学习是一种实现机器学习的技术
- 神经网络是一种机器学习算法

**常见算法**

![image.png](https://cdn.jsdelivr.net/gh/narugakuru/images/img/20230117194148.png)



### 机器学习：实现人工智能的方法
![image.png](https://cdn.jsdelivr.net/gh/narugakuru/images/img/20230117194930.png)

**机器学习有三类：**
第一类是无监督学习，指的是从信息出发自动寻找规律，并将其分成各种类别，有时也称"聚类问题"。

第二类是监督学习，监督学习指的是给历史一个标签，运用模型预测结果。如有一个水果，我们根据水果的形状和颜色去判断到底是香蕉还是苹果，这就是一个监督学习的例子。

最后一类为强化学习，是指可以用来支持人们去做决策和规划的一个学习方式，它是对人的一些动作、行为产生奖励的回馈机制，通过这个回馈机制促进学习，这与人类的学习相似，所以强化学习是目前研究的重要方向之一。


### 深度学习：实现机器学习的技术

机器学习同深度学习之间是有区别的，机器学习是指计算机的算法能够像人一样，从数据中找到信息，从而学习一些规律。虽然深度学习是机器学习的一种，但深度学习是利用深度的神经网络，将模型处理得更为复杂，从而使模型对数据的理解更加深入。

深度学习是机器学习中一种基于对数据进行表征学习的方法。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。

同机器学习方法一样，深度机器学习方法也有监督学习与无监督学习之分。不同的学习框架下建立的学习模型很是不同。例如，卷积神经网络（Convolutional neural networks，简称CNNs）就是一种深度的监督学习下的机器学习模型，而深度置信网（Deep Belief Nets，简称DBNs）就是一种无监督学习下的机器学习模型。


### 神经网络：机器学习算法

神经网络在设计的时候就是模仿人脑的处理方式，希望其可以按人类大脑的逻辑运行（尽管目前来说对人脑的研究仍不够透彻）。神经网络已经有很多年的历史，但现在基本很少听到了。饮鹿网（innov100）产业研究员认为神经网络可以简单的分为单层，双层，以及多层网络。神经网络在之前有非常多的问题，层数无法深入过多，有太多的参数需要调节，样本数据量过小等问题。总之，其之前是一门并不被看好的技术。直到2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。
<img src="https://cdn.jsdelivr.net/gh/narugakuru/images/img/image-20230117193705326.png" alt="image-20230117193705326" style="zoom: 50%;" />

### 人工智能的研究领域和分支

人工智能研究的领域主要有五层：

1、最底层是基础设施建设，包含数据和计算能力两部分，数据越大，人工智能的能力越强。

2、往上一层为算法，如卷积神经网络、LSTM 序列学习、Q-Learning、深度学习等算法，都是机器学习的算法。

3、第三层为重要的技术方向和问题，如计算机视觉，语音工程，自然语言处理等。还有另外的一些类似决策系统，像 reinforcement learning（编辑注：增强学习），或像一些大数据分析的统计系统，这些都能在机器学习算法上产生。

4、第四层为具体的技术，如图像识别、语音识别、机器翻译等等。

5、最顶端为行业的解决方案，如人工智能在金融、医疗、互联网、交通和游戏等上的应用，这是我们所关心它能带来的价值。


# 符号
## 数字
* $x$：标量
* $\mathbf{x}$：向量
* $\mathbf{X}$：矩阵
* $\mathsf{X}$：张量
* $\mathbf{I}$：单位矩阵
* $x_i$, $[\mathbf{x}]_i$：向量$\mathbf{x}$第$i$个元素
* $x_{ij}$, $[\mathbf{X}]_{ij}$：矩阵$\mathbf{X}$第$i$行第$j$列的元素

## 集合论
* $\mathcal{X}$: 集合
* $\mathbb{Z}$: 整数集合
* $\mathbb{R}$: 实数集合
* $\mathbb{R}^n$: $n$维实数向量集合
* $\mathbb{R}^{a\times b}$: 包含$a$行和$b$列的实数矩阵集合
* $\mathcal{A}\cup\mathcal{B}$: 集合$\mathcal{A}$和$\mathcal{B}$的并集
* $\mathcal{A}\cap\mathcal{B}$：集合$\mathcal{A}$和$\mathcal{B}$的交集
* $\mathcal{A}\setminus\mathcal{B}$：集合$\mathcal{A}$与集合$\mathcal{B}$相减，$\mathcal{B}$关于$\mathcal{A}$的相对补集

## 函数和运算符
* $f(\cdot)$：函数
* $\log(\cdot)$：自然对数
* $\exp(\cdot)$: 指数函数
* $\mathbf{1}_\mathcal{X}$: 指示函数
* $\mathbf{(\cdot)}^\top$: 向量或矩阵的转置
* $\mathbf{X}^{-1}$: 矩阵的逆
* $\odot$: 按元素相乘
* $[\cdot, \cdot]$：连结
* $\lvert \mathcal{X} \rvert$：集合的基数
* $\|\cdot\|_p$: ：$L_p$ 正则
* $\|\cdot\|$: $L_2$ 正则
* $\langle \mathbf{x}, \mathbf{y} \rangle$：向量$\mathbf{x}$和$\mathbf{y}$的点积
* $\sum$: 连加
* $\prod$: 连乘
* $\stackrel{\mathrm{def}}{=}$：定义

## 微积分
* $\frac{dy}{dx}$：$y$关于$x$的导数
* $\frac{\partial y}{\partial x}$：$y$关于$x$的偏导数
* $\nabla_{\mathbf{x}} y$：$y$关于$\mathbf{x}$的梯度
* $\int_a^b f(x) \;dx$: $f$在$a$到$b$区间上关于$x$的定积分
* $\int f(x) \;dx$: $f$关于$x$的不定积分

## 概率与信息论
* $P(\cdot)$：概率分布
* $z \sim P$: 随机变量$z$具有概率分布$P$
* $P(X \mid Y)$：$X\mid Y$的条件概率
* $p(x)$: 概率密度函数
* ${E}_{x} [f(x)]$: 函数$f$对$x$的数学期望
* $X \perp Y$: 随机变量$X$和$Y$是独立的
* $X \perp Y \mid Z$: 随机变量$X$和$Y$在给定随机变量$Z$的条件下是独立的
* $\mathrm{Var}(X)$: 随机变量$X$的方差
* $\sigma_X$: 随机变量$X$的标准差
* $\mathrm{Cov}(X, Y)$: 随机变量$X$和$Y$的协方差
* $\rho(X, Y)$: 随机变量$X$和$Y$的相关性
* $H(X)$: 随机变量$X$的熵
* $D_{\mathrm{KL}}(P\|Q)$: $P$和$Q$的KL-散度

## 复杂度
* $\mathcal{O}$：大O标记

[Discussions](https://discuss.d2l.ai/t/2089)